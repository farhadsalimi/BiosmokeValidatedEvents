#+TITLE:bio events 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----

* TODO-list

** TODO grant comments
Reading the manuscript - you state in section 2.3 a "weighted average" is being used to substitute missing data.  I assume this refers to step 3.2 but there's no mention there of what this weighting is.  Reading this, I would assume it would be some kind if area-weighted or IDW interpolation, but that's not the case is it?

In the additional files document, there probably needs to be a description of the pollution database schema, field names, types, table names etc. for the pollution data because this all seems to be pretty hard coded (including town names etc.) - I guess the intention is for people to take the code and modify it to suit their data, rather than use it as an immediately usable R package?  It would be quite a challenge for an independent researcher to take their monitor data, in whatever form it may be, and run this code on in it to perform the imputation.
* setup
- The idea is to house a complete data and document compendium as an R package
- Using the template from https://github.com/jhollist/manuscriptPackage

* threaded doc showing the methods, send methods to Rmd for both an EML document and a Data Note to BMC tex
** headers
#+begin_src R :session *R* :tangle inst/doc/methods.Rmd :exports none :eval no :padline no
  ---
  title: "Biosmoke Validated Events Database Protocols"
  author: Ivan C. Hanigan
  output:
    html_document:
      toc: false
      theme: united
      number_sections: no    
    pdf_document:
      toc: false
      toc_depth: 3
      highlight: zenburn
      keep_tex: true
      number_sections: no        
  documentclass: article
  classoption: a4paper
  csl: methods-in-ecology-and-evolution.csl
  bibliography: references.bib
  ---
  
  ```{r echo = F, eval=F, results="hide"}
#+end_src
** run-able R
#+begin_src R :session *R* :tangle inst/doc/methods.Rmd :exports none :eval yes :padline no
  setwd("~/projects/biomass_smoke_and_human_health/BiosmokeValidatedEvents")
  library(rmarkdown)
  library(knitr)
  library(knitcitations)
  library(bibtex)
  
  cleanbib()
  cite_options(citation_format = "pandoc", check.entries=FALSE)
  # to make editable
  rmarkdown::render("inst/doc/methods.Rmd", "word_document")
  
  # to make beautiful 
  #rmarkdown::render("inst/doc/methods.Rmd", "pdf_document")
  # if tex get out of doc because it breaks the R build
  #file.rename("inst/doc/methods.tex", "vignettes/methods.tex")
  # edit to remove latex header, copy abstract etc and urls
  #file.remove("inst/doc/methods.pdf")
  dir("vignettes")
  rmarkdown::render("vignettes/BiosmokeValidatedEvents_AdditionalFile1.Rmd", "word_document")
  setwd("..")
  system("pandoc()")
#+end_src

#+RESULTS:
: /home/ivan_hanigan/data/BiosmokeValidatedEvents/inst/doc/methods.pdf

** perhaps tangle out pure R
#+begin_src R :session *R* :tangle inst/doc/methods.Rmd :exports none :eval no :padline no
  ```
  ```{r echo = F, eval=F, results="hide"}  
  # to tangle chunks even when eval = F use this (with eval=F)
  library(knitr)
  knit_hooks$set(purl = function(before, options) {
    if (before) return()
    input  = current_input()  # filename of input document
    output = paste(tools::file_path_sans_ext(input), 'R', sep = '.')
    if (knitr:::isFALSE(knitr:::.knitEnv$tangle.start)) {
      assign('tangle.start', TRUE, knitr:::.knitEnv)
      unlink(output)
    }
    cat(options$code, file = output, sep = '\n', append = TRUE)
  })
  
  ```  
#+end_src
** bib
#+begin_src R :session *R* :tangle inst/doc/methods.Rmd :exports none :eval no :padline no
  
  ```{r, echo = F, results = 'hide'}
  # load
  if(!exists("bib")){
  bib <- read.bibtex("~/references/library.bib")
  }
  ```
  
#+end_src

** abstract
#+begin_src R :session *R* :tangle inst/doc/methods.Rmd :exports none :eval no :padline yes 
  # Abstract
  
  ,**Background:** Epidemiological studies of the health effects of
   biomass smoke events (such as bushfires or wood-heater smoke spikes
   due to inversion layers) have been hampered by the availability of
   datasets that explicitly pertain to these sources. Extreme air
   pollution events may also be caused by dust storms, fossil fuel
   induced smog events or factory fires. This paper presents an open and
   extensible database developed by the authors to identify historical
   spikes in PM concentrations and to evaluate whether they were caused
   by vegetation fire smoke or by other possible sources. These methods
   provide a systematic framework for retrospective identification of
   the air quality impacts of biomass smoke in a region that is
   seasonally affected by fires.  In this paper, we describe the
   database and data aquisition methods, as well as analytical
   considerations when validating historical events using a range of
   reference types.
  
  ,**Methods:** Several major urban centers and smaller regional towns in
   the Australian states of New South Wales, Western Australia, and
   Tasmania were selected as they are intermittently affected by extreme
   episodes of vegetation fire smoke.  Air pollution data was collated
   and missing values were imputed.  Extreme values were identified and
   a range of sources of reference information were assessed for each
   date.  Reference types online newspaper archives, government and
   research agency records, satellite imagery and a Dust Storms
   database.
  
  ,**Results:** This dataset contains validated events of extreme biomass
    smoke pollution across Australian cities. The authors have
    previously demonstrated the utility of this database in analyses of
    hospital admissions and mortality data for these locations to
    quantify the pollution-related health effects of these events.  
  
  ,**Conclusions:** The database was created using open source software
    and this makes the prospect for future extensions to the database
    possible.  THis is because if other scientists notice an ommision or
    error in these data they can offer an amendment. We believe that
    this will improve the database and benefit the whole biomass smoke
    health research community.
  
#+end_src
** findings
#+begin_src R :session *R* :tangle inst/doc/methods.Rmd :exports none :eval no :padline yes 
  # Findings
  
  ## Description
  
  The background and purpose of the database or data collection should
  be presented for readers without specialist knowledge in that area.
  For this database we should cite the original paper by
  `r citet(bib[["JohnstonFJHaniganICHendersonSBMorganGGandBowman"]])` as well as
  the two health analyses of Hospitalisation
  `r citep(bib[["Martin2013"]])` and Mortality
  `r citep(bib[["Johnston2011c"]])`.
  
  This will be followed by a brief description of the protocol for data collection, data curation
  and quality control, and what is being reported in the article.
  
  The user interface should be described and a discussion
  of the intended uses of the database, and the benefits that are
  envisioned, should be included, together with data on how it compares with similar
  existing databases. A case study of the use of the database may be
  presented. The planned future development of new features, if any,
  should be mentioned.
  
  The findings section can be broken into subsections with short
  informative headings. There is no maximum length for this section but
  we encourage authors to be concise.
  
  ## General Protocols
  
  For each location, up to 13 yr (between 1994 and 2007) of daily air
  quality data measured asPMless than 10um (PM10 ) or less than 2.5 um
  (PM2.5 ) in aerodynamic diameter were examined. Air pollution data
  were pro- vided by government agencies in the states of Western
  Australia, New South Wales, and Tasmania. Daily averages for each site
  were calculated excluding days with less than 75% of hourly
  measurements. In Sydney and Perth, where data were collected from
  several monitoring sta- tions, the missing daily site-specific PM10
  and PM2.5 con- centrations were imputed using available data from
  other proximate monitoring sites in the network. The daily city-wide
  PM10 and PM2.5 concentrations were then estimated following the
  protocol of the Air Pollution and Health: a European Approach studies
  `r citep(bib[["Atkinson2001"]])`. 
#+end_src
** method steps
*** step1 source air pollution data and load to a postgres database
http://www.environment.nsw.gov.au/AQMS/sitesyd.htm
#+begin_src R :session *R* :tangle inst/doc/methods.Rmd :exports none :eval no :padline yes 
  ## Detailed Data Collation and Validation Methods
  
  First a 'filling-in' procedure was used to improve data
  completeness. It entailed the substitution of the missing daily values
  with a weighted average, using the weights of the missing sites
  3-month average proportional to the network average. The weights are
  calculated against the values from the rest of the monitoring
  stations. The pollutant measures from all stations providing data were
  then averaged to provide single, city-wide estimates of the daily
  levels of the pollutants
  
  For each city, all days in which PM10 or PM2.5 exceeded the 95th
  percentile were identified over the entire time series. These extreme
  values were termed 'events'. A range of sources was ex- amined to
  identify the cause of particulate air pollution events, including
  electronic news archives, Internet searches for other reports,
  government and research agencies, satellite imagery and a Dust Storms
  database. Also examined were remotely sensed aerosol optical thickness
  (AOT) data to provide further information about days for which the
  other methods did not.
  
  Step 1. Source air pollution data. Both time series observations and spatial data regarding site locations. 
  
  Step 1.1. NSW data downloaded from an online data server.  Site locations (Lat and Long) obtained from website.
  
  Step 1.2. WA data sent on CD from contacts at the WA Government Department, these were hourly data as provided.  Cleaned so as only days with >75% of hours are used.  Licence puts restricions on our right to provide to a third party.  Therefore those observed and imputed data are not included, only the events.
  
  Step 1.3. Tasmanian data sent via email from contact at the Department, these were daily data.
  
  Step 1.4. All data combined and Quality Control checked in the PostGIS database.  
  
#+end_src
**** COMMENT data prep
see ~/projects/biomass_smoke_and_human_health/biosmoke_pollution

*** step2 source and load spatial data representing the city boundaries
#+begin_src R :session *R* :tangle inst/doc/methods.Rmd :exports none :eval no :padline yes 

Step 2. Spatial data for cities.

#+end_src
**** COMMENT data prep
see ~/projects/biomass_smoke_and_human_health/biosmoke_spatial

*** step3 Calculate a network average
#+begin_src R :session *R* :tangle inst/doc/methods.Rmd :exports none :eval no :padline yes 
  
  Step 3. Calculate a network average. In cities where data were
  collected from several monitoring stations, the missing daily
  site-specific PM concentrations were imputed using available data from
  other proximate monitoring sites in the network. The daily city-wide
  PM concentrations were then estimated following the protocol of the
  Air Pollution and Health: a European Approach
  studies. `r citet(bib[["Atkinson2001"]])`.
  
  Step 3.1. Prepare Data.  First it was necessary to find the minimum
  date that the series of continuous observations can be considered to
  start.  In the Australian datasets the initial observations could not
  be used because the were sometimes only one day per week, only during
  a particular season or of poor quality due to teething problems with
  equipment and procedures.  Then it was necessary to identify missing
  dates.  Get a list of the sites to include – that is with more than 70%
  observed over the time period (as defined after assessing min and max
  dates of period).
#+end_src

**** main
#+begin_src R :session *R* :tangle inst/doc/main.R :exports none :padline no :eval no
  #################################################################
  projectdir <- "~/projects/biomass_smoke_and_human_health/BiosmokeValidatedEvents/inst/doc"
  setwd(projectdir)
  library(rpostgrestools)
  # you will need to request username and password
  ch <- connect2postgres2("ewedb_staging")
  
#+end_src
**** main setup list of towns
#+begin_src R :session *R* :tangle inst/doc/main.R :exports none :padline no :eval no
    
  
  #################################################################
  towns <- c("PERTH", "Sydney","Illawarra","Lower Hunter","Hobart","Launceston")  
  
#+end_src

#+RESULTS:
| PERTH        |
| Sydney       |
| Illawarra    |
| Lower Hunter |
| Hobart       |
| Launceston   |

**** main setup list of pollutants
#+begin_src R :session *R* :tangle inst/doc/main.R :exports none :padline no :eval no
    
  
  #################################################################
  
  # list pollutants
  polls <- cbind(c("sulphurdioxide_pphm","nitrogendioxide_pphm",
                   "carbonmonoxide_ppm","ozone_pphm","particulatematter10um_ugm3",
                   "nephelometer_bsp", "particulatematter2_5um_ugm3", "nitricoxide_pphm"),
                 c("so2_max","no2_max","co_max" ,    "o3_max", "pm10_av", "bsp_max",
                   "pm25_av",  "no_max"),
                 c("SO2","NO2","CO","O3","PM10","BSP","PM25","NO")
                 )
  polls
  # select on for this run
  poll_i <- 7
  (poll <- polls[poll_i,3])
  (pollutant <- polls[poll_i,2])
  
#+end_src

#+RESULTS:
: o3_max

**** func mindates_pollutant-code
#+name:mindates_pollutant
#+begin_src R :session *R* :tangle R/mindates_pollutant.R :exports none :padline no :eval yes
  #' @name mindates_pollutant
  #' @title Minimum Date a Pollutant is observed from 
  #' @param town the Biomass Study Town in question
  #' @param pollutant you got it
  #' @return text for a SQL query
  
  
  mindates_pollutant <- function(
    town = "perth"
    ,
    pollutant = "pm10_av"
    ){
    if(length(grep("_av$", pollutant)) > 0){
      pollutant_label <- gsub("_av$", "_avg", pollutant)
    } else {
      pollutant_label <- pollutant
    }
  txt <- paste("select t1.r2, min(t1.date) as min",pollutant_label,"
        from (
          SELECT combined_pollutants2.r2, date, avg(",pollutant,") as ", pollutant_label, "
          FROM biosmoke_pollution.combined_pollutants 
          join 
          (
                  select t1.site,t1.region as r2, t2.studysite as region
                  from biosmoke_pollution.pollution_stations_combined_final t1,
                  biosmoke_spatial.study_slas_01 t2
                  where st_intersects(t1.geom,t2.geom)
                    and lower(
        case when t2.studysite like \'Sydney%\' then \'Sydney\' else t2.studysite end 
                     ) = \'",tolower(town),"\'
                  order by studysite
          ) combined_pollutants2 
          on biosmoke_pollution.combined_pollutants.site = combined_pollutants2.site
          where ",pollutant," is not null
          group by r2,date
          order by r2, date) t1
        group by t1.r2
    ", sep = "")
  #cat(txt)
    return(txt)
  }
  
#+end_src

#+RESULTS: mindates_pollutant
**** func all_stations_all_dates-code
#+name:all_stations_all_dates
#+begin_src R :session *R* :tangle R/all_stations_all_dates.R :exports none :padline no :eval yes
  #' @name all_stations_all_dates
  #' @title All Stations, All Dates
  #' @param town Biomass Study area
  #' @param pollutant you got it
  #' @return text for a query
  all_stations_all_dates <- function(town, pollutant){
  if(length(grep("_av$", pollutant)) > 0){
    pollutant_label <- gsub("_av$", "", pollutant)
  } else {
    pollutant_label <- pollutant
  }
  
  txt <- paste("
  select site as station, date 
  into biosmoke_pollution.stationdates_",town,"_",pollutant_label,"
  from
  (select distinct biosmoke_pollution.combined_pollutants.site 
  from biosmoke_pollution.combined_pollutants
  join
          (
          select t1.site,t2.studysite as region
          from biosmoke_pollution.pollution_stations_combined_final t1 , 
          biosmoke_spatial.study_slas_01 t2
          where st_intersects(t1.geom,t2.geom) and upper(t2.studysite) like '",toupper(town),"%'
          order by studysite
          ) combined_pollutants2
  on biosmoke_pollution.combined_pollutants.site=combined_pollutants2.site
  ) sites,
  (select * from alldates_",pollutant_label,"_",town,") dates
  ",sep="")
  
  # cat(txt)
  return(txt)
  }
#+end_src

#+RESULTS: all_stations_all_dates

**** missing dates are added for completeness
#+begin_src R :session *R* :tangle inst/doc/01_prepare_dates.R :exports none :padline no :eval no
  #################################################################
  # to identify sites to be included need to know how many missing days.
  # first create complete set of statoiondates for the sites per town
  # this was set up after assessing the time series for completeness.  
  # Perth and Launceston PM10 mindates were altered 
  matrix(towns)
  ## [1,] "PERTH"       
  ## [2,] "Sydney"      
  ## [3,] "Illawarra"   
  ## [4,] "Lower Hunter"
  ## [5,] "Hobart"      
  ## [6,] "Launceston"  
  
  # note o3 only done for towns[1:4]
  
  for(town in towns){
  #town  <- towns[2]
  # housekeeping code to begin
  # NB the updates made in 2015 mean that the 2007 end date is no longer
  # correct in sydney
  if(town == "Sydney"){
      maxdate_selected  <- as.Date('2014-12-31')
  }  else {
      maxdate_selected  <- as.Date('2007-12-31')
  }
  
    
  # town=towns[4]
  # for hunter make it newcastle
          if( town == "Lower Hunter"){
          town='Newcastle'
          }
  # town=towns[2]
  print(town)
  
  txt <- mindates_pollutant(town = town, pollutant = "pm10_av")
  mindatesp10 <- dbGetQuery(ch, txt)
  mindatesp10
  txt <- mindates_pollutant(town = town, pollutant = "pm25_av")
  mindatesp25 <- dbGetQuery(ch, txt)
  mindatesp25
  txt <- mindates_pollutant(town = town, pollutant = "o3_max")
  # and this one is seperate because it fails in towns without o3  
  mindateo3 <- dbGetQuery(ch, txt)
  mindateo3
   
  # TODO it would be nice to include a user interaction stage, where the start date could be modified  
  # Need to change for perth pm10 mindate because of duncraig monitoring station
  if(town == "PERTH"){
  mindatesp10[,2] <- as.Date('1997-05-23')
  }
  
  # in Launceston change pm10 mindate ="'1997-05-09'" changed from "'1992-05-04'" as this is start of consecutive day measurements prior to that it was weekly and seasonal
  if(town == "Launceston"){
  mindatesp10[,2] <- as.Date('1997-05-09')
  }
  
  #### PM10
  # max date is 2007, make a table with all dates 
  alldates_pm10_town  <- as.data.frame(as.Date(mindatesp10[,2]:maxdate_selected,'1970-01-01'))
  alldates_pm10_town$id <- 1:nrow(alldates_pm10_town)
  names(alldates_pm10_town) <- c('date','id')
  dbWriteTable(ch, paste('alldates_pm10_',tolower(town),sep=''), alldates_pm10_town, row.names = F)
  
  # make a table with every date at every station  
  txt <- all_stations_all_dates(town = town, pollutant = "pm10_av")
  #cat(txt)
  # try to be tidy
  try(
  dbSendQuery(ch,paste("drop table biosmoke_pollution.stationdates_",town,"_pm10;",sep=''))
  )
  dbSendQuery(ch, txt)
  dbSendQuery(ch,
  paste('drop table alldates_pm10_',town,sep='')
  )
  
  #### PM2.5
  # max date is 2007, make a table with all dates 
  alldates_pm25_town  <- as.data.frame(as.Date(mindatesp25[,2]:maxdate_selected,'1970-01-01'))
  alldates_pm25_town$id <- 1:nrow(alldates_pm25_town)
  names(alldates_pm25_town) <- c('date','id')
  dbWriteTable(ch, paste('alldates_pm25_',tolower(town),sep=''), alldates_pm25_town, row.names = F)
  
  # make a table with every date at every station  
  txt <- all_stations_all_dates(town = town, pollutant = "pm25_av")
  #cat(txt)
  # try to be tidy
  try(
  dbSendQuery(ch,paste("drop table biosmoke_pollution.stationdates_",town,"_pm25;",sep=''))
  )
  dbSendQuery(ch, txt)
  dbSendQuery(ch,
  paste('drop table alldates_pm25_',town,sep='')
  )
  
  #### O3
  # max date is 2007, make a table with all dates
  if(nrow(mindateo3) > 0){        
  alldates_o3_town  <- as.data.frame(as.Date(mindateo3[,2]:maxdate_selected,'1970-01-01'))
  alldates_o3_town$id <- 1:nrow(alldates_o3_town)
  names(alldates_o3_town) <- c('date','id')
  dbWriteTable(ch, paste('alldates_o3_',tolower(town),sep=''), alldates_o3_town, row.names = F)
  
  # make a table with every date at every station  
  txt <- all_stations_all_dates(town = town, pollutant = "o3_av")
  #cat(txt)
  # try to be tidy
  try(
  dbSendQuery(ch,paste("drop table biosmoke_pollution.stationdates_",town,"_o3;",sep=''))
  )
  dbSendQuery(ch, txt)
  dbSendQuery(ch,
  paste('drop table alldates_o3_',town,sep='')
  )
  }
          
  }
  
  
#+end_src

**** main missing dates-code
#+name:main missing dates
#+begin_src R :session *R* :tangle inst/doc/main.R :exports none :padline no :eval no
#### Do the processing
source("01_prepare_dates.R")
#+end_src

*** a lot of functions and a loop
#+begin_src R :session *R* :tangle inst/doc/methods.Rmd :exports none :eval no :padline yes 
  
  Step 3.2. Loop over each station individually and calculate a daily
  network average of all the other non-missing sites (ie an average of
  all stations except the focal station of that iteration in the
  loop).

  Step 3.3. Calculate three monthly seasonal mean of these non-missing
  stations.  Calculate a three-month seasonal mean for MISSING site.
  Estimate missing days at missing sites.
  
  Step 3.4. Join all sites for city wide averages and fill any missing days with
  avg of before and after.
  
  Step 3.5 Take the average of all sites per day for city wide averages.
  
  Step 3.6. Fill any missing days with avg of before and after (if this is less than 5% of days).
  
#+end_src

#+end_src
**** R setup the mindates/polls/towns
#+begin_src R :session *R* :tangle inst/doc/main.R :exports none :padline no :eval no
  
  #### Set up a list of things to do in order ####
  todo=cbind(towns,rep('pm10',length(towns)),c("'1997-05-23'","'1994-01-01'","'1994-02-15'",
  "'1994-02-02'","'2006-04-22'" ,"'2001-05-01'"))
  
  todo=rbind(todo,cbind(towns,rep('pm25',length(towns)),c("'1994-02-15'","'1996-05-07'","'1998-03-01'" ,"'1996-06-19'","'2006-06-05'" ,"'2005-06-04'")))
  
  todo=rbind(todo,cbind(towns[1:4],rep('o3',4),rep("'1994-01-01'",4)))
  
  todo=as.data.frame(todo)
  todo
  todo$stat=ifelse(todo[,2]=='o3','max','av')
  todo
  
  i=8
  todo[i,]
  town=todo[i,1]
  poll=todo[i,2]
  mindate="'2003-01-01'"
    #todo[i,3]
  stat=todo[i,4]
     
#+end_src
**** view todo
          towns   V2           V3 stat
1         PERTH pm10 '1997-05-23'   av
2        Sydney pm10 '1994-01-01'   av
3     Illawarra pm10 '1994-02-15'   av
4  Lower Hunter pm10 '1994-02-02'   av
5        Hobart pm10 '2006-04-22'   av
6    Launceston pm10 '2001-05-01'   av
7         PERTH pm25 '1994-02-15'   av
8        Sydney pm25 '1996-05-07'   av
9     Illawarra pm25 '1998-03-01'   av
10 Lower Hunter pm25 '1996-06-19'   av
11       Hobart pm25 '2006-06-05'   av
12   Launceston pm25 '2005-06-04'   av
13        PERTH   o3 '1994-01-01'  max
14       Sydney   o3 '1994-01-01'  max
15    Illawarra   o3 '1994-01-01'  max
16 Lower Hunter   o3 '1994-01-01'  max

**** COMMENT func sites_todo-code
#+name:func sites_todo
#+begin_src R :session *R* :tangle R/sites_todo.R :exports none :padline no :eval yes
  #' @name sites_todo
  #' @title sites with potential
  #' @param town
  #' @param mindate
  #' @param maxdate
  #' @param threshold
  #' @param poll
  #' @param stat
  #' @return text for a sql query
  
  sites_todo <- function(town, mindate, maxdate="2007-12-31", threshold=0.7, poll, stat){
  
  print(poll);print(town)
  print(stat)
  # av or max?
  
  # find the stations with complete
  txt <- paste("
  select site,count,count(*) as potential, cast(count as numeric)/cast(count(*) as numeric) as complete
  from
          (
          select polls.* , valid.count,mindate.*
          from 
          (
                  (
                  SELECT biosmoke_pollution.stationdates_",town,"_",poll,".station as site, biosmoke_pollution.stationdates_",town,"_",poll,".date, ",poll,"_",stat," as param
                  FROM
                  biosmoke_pollution.stationdates_",town,"_",poll,"
                  left join
                  biosmoke_pollution.combined_pollutants
                  on biosmoke_pollution.stationdates_",town,"_",poll,".station=biosmoke_pollution.combined_pollutants.site
                  and biosmoke_pollution.stationdates_",town,"_",poll,".date=biosmoke_pollution.combined_pollutants.date
                  ) polls
          join 
                  (
                  SELECT biosmoke_pollution.stationdates_",town,"_",poll,".station as site, count(",poll,"_",stat,"), min(biosmoke_pollution.combined_pollutants.date)
                  FROM
                  biosmoke_pollution.stationdates_",town,"_",poll,"
                  left join
                  biosmoke_pollution.combined_pollutants
                  on biosmoke_pollution.stationdates_",town,"_",poll,".station=biosmoke_pollution.combined_pollutants.site
                  and biosmoke_pollution.stationdates_",town,"_",poll,".date=biosmoke_pollution.combined_pollutants.date
                  where ",poll,"_",stat," is not null and biosmoke_pollution.stationdates_",town,"_",poll,".date >= ",mindate,"
                                          and biosmoke_pollution.stationdates_",town,"_",poll,".date <= '",maxdate,"'
                  group by biosmoke_pollution.stationdates_",town,"_",poll,".station
                  ) valid
          on polls.site=valid.site
           
          ),
                  (
                  SELECT  min(biosmoke_pollution.combined_pollutants.date), max(biosmoke_pollution.combined_pollutants.date)
                  FROM
                  biosmoke_pollution.stationdates_",town,"_",poll,"
                  left join
                  biosmoke_pollution.combined_pollutants
                  on biosmoke_pollution.stationdates_",town,"_",poll,".station=biosmoke_pollution.combined_pollutants.site
                  and biosmoke_pollution.stationdates_",town,"_",poll,".date=biosmoke_pollution.combined_pollutants.date
                  where ",poll,"_",stat," is not null
                  ) mindate
          where polls.date >= ",mindate," and polls.date <= '",maxdate,"'
          order by polls.date
          ) foo
  group by site, count
  having cast(count as numeric)/cast(count(*) as numeric) >=",threshold,"
  ",sep="")
  
  # cat(txt)
  #d<- dbGetQuery(ch, txt)
  #sitelist <- d$site
  return(txt)
  }
  
  
#+end_src

#+RESULTS: func

**** R sites_todo
#+begin_src R :session *R* :tangle inst/doc/02_loop_over_stations_calculate_net_avg.R :exports none :padline no :eval no
  #### sites_todo
  txt <- sites_todo(town=town,mindate=mindate,poll=poll,stat=stat, maxdate = maxdate_selected)
  cat(txt)
  sitelist <- dbGetQuery(ch, txt)[,1]
  sitelist
  
#+end_src
**** COMMENT func impute
#+name:func sites_todo
#+begin_src R :session *R* :tangle R/impute.R :exports none :padline no :eval yes
  #' @name impute
  #' @title impute for each site
  #' @param sitelist sites
  #' @param town town
  #' @param poll pollutant
  #' @param stat statistical unit as per avg or max
  #' @param maxdate the end of the time series
  #' @return database table
  
  impute <- function(
    sitelist = c( "SouthLake", "Duncraig" )
    ,
    town = "PERTH"
    ,
    poll = "pm10"
    ,
    stat = "av"
    ,
    maxdate = "2007-12-31"
    ){
  
  # first make a table
  try(dbSendQuery(ch,
  # cat(
  paste("drop TABLE biosmoke_pollution.imputed_",poll,"_",town,sep='')
  ),silent=T)
  
  
  dbSendQuery(ch,
  # cat(
  paste("CREATE TABLE biosmoke_pollution.imputed_",poll,"_",town,"
  (
    site character varying(255),
    rawdate date,
    rawdata double precision,
    networkavg double precision,
    missingavg3mo double precision,
    networkavg3mo double precision,
    imputed double precision,
    imputed_param double precision
  )",sep="")
  )
  
  
  for(loc in sitelist[1:length(sitelist)]){
  # loc=sitelist[2]
  print(loc)
  
  # a) calculate a daily network average of all non-missing sites 
  txt <- paste("select date, avg(param) as networkavg         
  into biosmoke_pollution.networkavg
  from 
  (",
  paste("
  SELECT biosmoke_pollution.stationdates_",town,"_",poll,".station as site, biosmoke_pollution.stationdates_",town,"_",poll,".date, ",poll,"_",stat," as param
  FROM
  biosmoke_pollution.stationdates_",town,"_",poll,"
  left join
  biosmoke_pollution.combined_pollutants
  on biosmoke_pollution.stationdates_",town,"_",poll,".station=biosmoke_pollution.combined_pollutants.site
  and biosmoke_pollution.stationdates_",town,"_",poll,".date=biosmoke_pollution.combined_pollutants.date
  where biosmoke_pollution.stationdates_",town,"_",poll,".station = '",sitelist[-grep(loc,sitelist)],"'
                          and biosmoke_pollution.stationdates_",town,"_",poll,".date >= ",mindate," and biosmoke_pollution.stationdates_",town,"_",poll,".date <= '",maxdate,"'
  ",sep="",collapse="union"),
  ") t1
  where param is not null
  group by date
  order by date",sep="")
  
  #cat(txt)
  
  #strt=Sys.time()
  dbSendQuery(ch,txt)
  #endd=Sys.time()
  #print(endd-strt)
  
  # b) calculate a 3-month seasonal mean for this average of all non-missing sites
  
  # NB -45 and + 44 after reading the SAS CMOVAVE info as this is what it does when given an even number (90)
  txt <- "select t1.date, avg(t2.networkavg) as networkavg3mo          
  into biosmoke_pollution.networkavg3mo
  from
  biosmoke_pollution.networkavg t1,
  biosmoke_pollution.networkavg t2
  where (t2.date >= (t1.date -45) and t2.date <= (t1.date+44))
  group by t1.date 
  having count(t2.networkavg)>=(90*0.75)
  order by t1.date"
  
  #strt=Sys.time()
  dbSendQuery(ch,txt)
  #endd=Sys.time()
  #print(endd-strt)
  
  
  # c) calculate a 3-month seasonal mean for MISSING site
  
  txt <- paste("select t1.date, avg(t2.param) as missingavg3mo       
  into biosmoke_pollution.missingavg3mo
  from 
  (
  SELECT biosmoke_pollution.stationdates_",tolower(town),"_",poll,".station as site, biosmoke_pollution.stationdates_",tolower(town),"_",poll,".date, ",poll,"_",stat," as param
  FROM
  biosmoke_pollution.stationdates_",tolower(town),"_",poll,"
  left join
  biosmoke_pollution.combined_pollutants
  on biosmoke_pollution.stationdates_",tolower(town),"_",poll,".station=biosmoke_pollution.combined_pollutants.site
  and biosmoke_pollution.stationdates_",tolower(town),"_",poll,".date=biosmoke_pollution.combined_pollutants.date
  where biosmoke_pollution.stationdates_",tolower(town),"_",poll,".station = '",sitelist[grep(loc,sitelist)],"'
                          and biosmoke_pollution.stationdates_",tolower(town),"_",poll,".date >= ",mindate," and biosmoke_pollution.stationdates_",tolower(town),"_",poll,".date <= '",maxdate,"'
  ) t1,
  (
  SELECT biosmoke_pollution.stationdates_",tolower(town),"_",poll,".station as site, biosmoke_pollution.stationdates_",tolower(town),"_",poll,".date, ",poll,"_",stat," as param
  FROM
  biosmoke_pollution.stationdates_",tolower(town),"_",poll,"
  left join
  biosmoke_pollution.combined_pollutants
  on biosmoke_pollution.stationdates_",tolower(town),"_",poll,".station=biosmoke_pollution.combined_pollutants.site
  and biosmoke_pollution.stationdates_",tolower(town),"_",poll,".date=biosmoke_pollution.combined_pollutants.date
  where biosmoke_pollution.stationdates_",tolower(town),"_",poll,".station = '",sitelist[grep(loc,sitelist)],"'
                          and biosmoke_pollution.stationdates_",tolower(town),"_",poll,".date >= ",mindate," and biosmoke_pollution.stationdates_",tolower(town),"_",poll,".date <= '",maxdate,"'
  ) t2
  where (t2.date >= (t1.date -45) and t2.date <= (t1.date+44))
  group by t1.date 
  having count(t2.param)>=(90*0.75)",sep="")
  
  # cat(txt)
  strt=Sys.time()
  dbSendQuery(ch,txt)
  endd=Sys.time()
  print(endd-strt)
  
  # d) estimate missing days at missing sites and insert to output table
  txt <- paste("INSERT INTO  biosmoke_pollution.imputed_",poll,"_",tolower(town),"  (
              site, rawdate, rawdata, networkavg, missingavg3mo, networkavg3mo, 
              imputed, imputed_param
                                                  )
  select raw.site, raw.date as rawdate, param as rawdata, networkavg, missingavg3mo, networkavg3mo, 
              imputed, case when param is null then imputed else param end as imputed_param 
  from
  (
  SELECT biosmoke_pollution.stationdates_",tolower(town),"_",poll,".station as site, biosmoke_pollution.stationdates_",tolower(town),"_",poll,".date, ",poll,"_",stat," as param
                  FROM
                  biosmoke_pollution.stationdates_",tolower(town),"_",poll,"
                  left join
                  biosmoke_pollution.combined_pollutants
                  on biosmoke_pollution.stationdates_",tolower(town),"_",poll,".station=biosmoke_pollution.combined_pollutants.site
                  and biosmoke_pollution.stationdates_",tolower(town),"_",poll,".date=biosmoke_pollution.combined_pollutants.date
                                  where biosmoke_pollution.stationdates_",tolower(town),"_",poll,".date >= ",mindate,"
                                          and biosmoke_pollution.stationdates_",tolower(town),"_",poll,".date <= '",maxdate,"'
                                          and biosmoke_pollution.stationdates_",tolower(town),"_",poll,".station = '",loc,"'
  order by biosmoke_pollution.stationdates_",tolower(town),"_",poll,".date
  ) raw
  left join
  (
  select t1.date,
          t1.networkavg,
          t2.missingavg3mo,
          t3,networkavg3mo,
          t1.networkavg*(t2.missingavg3mo/t3.networkavg3mo) as imputed
  from ((biosmoke_pollution.networkavg t1
  join
          biosmoke_pollution.missingavg3mo t2
          on t1.date=t2.date)
  join
          biosmoke_pollution.networkavg3mo t3
          on t1.date=t3.date)
  order by t1.date
  ) imputed
  on raw.date=imputed.date
  order by raw.date
  ",sep="")
  
  #cat(txt)
  strt=Sys.time()
  dbSendQuery(ch,txt)              
  endd=Sys.time()
  print(endd-strt)
  
  
  dbSendQuery(ch,"drop table biosmoke_pollution.networkavg ;")
  dbSendQuery(ch,"drop table biosmoke_pollution.missingavg3mo;")
  dbSendQuery(ch,"drop table biosmoke_pollution.networkavg3mo;")
  
  }
  
  dbSendQuery(ch,
  # cat(
  paste("ALTER TABLE biosmoke_pollution.imputed_",poll,"_",town," rename rawdate to date",sep='')
  )
  
  }
  
  
#+end_src
**** R impute
#+begin_src R :session *R* :tangle inst/doc/02_loop_over_stations_calculate_net_avg.R :exports none :padline no :eval no
  impute(sitelist, town, poll, stat, maxdate = maxdate_selected)
  
#+end_src
**** R impute_aphea2
***** R-R impute_aphea2
#+name:R impute_aphea2
#+begin_src R :session *R* :tangle R/impute_aphea2.R :exports none :eval no
################################################################
# name:R impute_aphea2

#+end_src
***** test-R impute_aphea2
#+name:R impute_aphea2
#+begin_src R :session *R* :tangle tests/test-impute_aphea2.r :exports none :eval no
################################################################
# name:R impute_aphea2

#+end_src

**** COMMENT func n_missing
#+name:func sites_todo
#+begin_src R :session *R* :tangle R/n_missing.R :exports none :padline no :eval yes
  #' @name n_missing
  #' @title number missing
  #' @param town the one to do
  #' @param poll pollutant
  #' @param thresh theshold below which we will do it
  #' @return nmissing is a message like 'go for it'
  
  n_missing <- function(town,poll,thresh=0.05){
  
  nmissing<- dbGetQuery(ch,
  # cat(
  paste("
  select count(*) from
  (
  select 
   t1.date, avg(t2.",poll,") as citywide_",poll," , count(*)
  from
          (
          select date , avg(imputed_param) as ",poll,"
          from biosmoke_pollution.imputed_",poll,"_",town,"
          group by date
          having avg(imputed_param) is null
          ) t1,
          (
          select date , avg(imputed_param) as ",poll,"
          from biosmoke_pollution.imputed_",poll,"_",town,"
          group by date
          ) t2
  where (t2.date >= t1.date-1 and  t2.date <= t1.date+1)
  group by t1.date
  having count(t2.",poll,")>1
  order by t1.date
  ) foo
  ",sep="")
  )
  
  noverall<- dbGetQuery(ch,
  #cat(
  paste("select count(*) from
  (
  select date , avg(imputed_param) as ",poll,"
  from biosmoke_pollution.imputed_",poll,"_",town,"
  group by date
  ) bar",sep="")
  )
  
  if(nmissing/noverall<=thresh){"go for it"} else {"don't do the avg of the missing dates with before and after, too many"}
  
  }
  
  
  
#+end_src
**** R n_missing
#+begin_src R :session *R* :tangle inst/doc/02_loop_over_stations_calculate_net_avg.R :exports none :padline no :eval no
  # no avg all sites per day for city wide averages  
  # AND fill any missing days with avg of before and after (if this is less than 5% of days)
  # first make sure the number of missing days with one valid either side is < 5% of total days
  n_missing(town,poll)
  
  # if = 'go for it'
  
#+end_src
**** COMMENT func citywide_av
#+name:func sites_todo
#+begin_src R :session *R* :tangle R/citywide_av.R :exports none :padline no :eval yes
  #' @name citywide_av
  #' @title city wide average
  #' @param town
  #' @param poll
  #' @param stat
  #' @return nothing to R, this creates things in the database
  citywide_av <- function(town, poll, stat){
  
  # calculate and insert to temp table
  try(dbSendQuery(ch,
  #cat(
  paste("drop TABLE biosmoke_pollution.",poll,"_",stat,"_events_",town,"_temp",sep='')
  ),silent=T)
  
  dbSendQuery(ch,
  #cat(
  paste("CREATE TABLE biosmoke_pollution.",poll,"_",stat,"_events_",town,"_temp
  (
    date date NOT NULL,
    ",poll,"_",stat," numeric,
    ranked serial
  )",sep="")
  )
  
  dbSendQuery(ch,
  #cat(
  paste("
  INSERT INTO biosmoke_pollution.",poll,"_",stat,"_events_",town,"_temp (
      date, ",poll,"_",stat,")
  select citywide.date,
          case when citywide.",poll," is null then citywide_",poll," else ",poll," end as citywide_",poll,"
  from
          (
          select date , avg(imputed_param) as ",poll,"
          from biosmoke_pollution.imputed_",poll,"_",town,"
          group by date
          ) citywide
  left join
          (
          select 
                  t1.date, avg(t2.",poll,") as citywide_",poll," , count(*)
          from
                  (
                  select date , avg(imputed_param) as ",poll,"
                  from biosmoke_pollution.imputed_",poll,"_",town,"
                  group by date
                  having avg(imputed_param) is null
                  ) t1
          ,
                  (
                  select date , avg(imputed_param) as ",poll,"
                  from biosmoke_pollution.imputed_",poll,"_",town,"
                  group by date
                  ) t2
          where (t2.date >= t1.date-1 and  t2.date <= t1.date+1)
          group by t1.date
          having count(t2.",poll,")>1
          order by t1.date
          ) impute_missing_days
  on citywide.date=impute_missing_days.date
  where case when citywide.",poll," is null then citywide_",poll," else ",poll," end is not null
  order by case when citywide.",poll," is null then citywide_",poll," else ",poll," end
  ",sep="")
  )
  
  # ok calculate % and insert to output table
  try(dbSendQuery(ch,
  #cat(
  paste("drop TABLE biosmoke_pollution.",poll,"_",stat,"_events_",town,sep="")
  ),silent=T)
  
  
  
  dbSendQuery(ch,
  #cat(
  paste("CREATE TABLE biosmoke_pollution.",poll,"_",stat,"_events_",town,"
  (
    date date NOT NULL,
    ",poll,"_",stat," numeric,
    ranked numeric,
    pctile numeric
  )",sep="")
  )
  
  dbSendQuery(ch,
  #cat(
  paste("
  INSERT INTO biosmoke_pollution.",poll,"_",stat,"_events_",town," (
              date, ",poll,"_",stat,",ranked,pctile)
  select *, (cast(ranked as numeric)-1)/(
          (
          select count(*) from biosmoke_pollution.",poll,"_",stat,"_events_",town,"_temp
          ) 
  -1) as pctile
  from biosmoke_pollution.",poll,"_",stat,"_events_",town,"_temp",sep="")
  )
  
  }
  
#+end_src
**** R citywide_av
#+begin_src R :session *R* :tangle inst/doc/02_loop_over_stations_calculate_net_avg.R :exports none :padline no :eval no
  
  citywide_av(town,poll,stat)
#+end_src
**** R loop over all towns
#+begin_src R :session *R* :tangle inst/doc/02_loop_over_stations_calculate_net_avg.R :exports none :padline no :eval no
  for(i in 2:nrow(todo)){
  # i=15
  town=todo[i,1]
  if(town=="Lower Hunter"){
          town='Newcastle'
          } else {
          town=todo[i,1]
          }
  print(town)     
  poll=todo[i,2]
  print(poll)
  mindate=todo[i,3]
  print(mindate)
  stat=todo[i,4]
  print(stat)
  
  txt <- sites_todo(town=town,mindate=mindate,poll=poll,stat=stat, maxdate = maxdate_selected)
  sitelist <- dbGetQuery(ch, txt)[,1]
  #sitelist
  
  impute(sitelist, town, poll, stat, maxdate = maxdate_selected)
  
  
  nmissed=n_missing(town,poll)
  print(nmissed)
  if(nmissed=='go for it'){
          citywide_av(town,poll,stat)
          }
          
  }
#+end_src  
**** COMMENT func stitch_together
#+name:func sites_todo
#+begin_src R :session *R* :tangle R/stitch_together.R :exports none :padline no :eval yes
  #' @name stitch_together
  #' @title put all the bits together
  #' @param poll pollutant
  #' @param stat av or max
  #' @return tables in the database
  stitch_together <- function(poll=polls[5,3], stat = 'av'){
  
  print(poll)
  
  # NB only once!
  try(
  exist<- dbGetQuery(ch,
  #cat(
  paste("select * from biosmoke_pollution.",poll,"_",stat,"_events_all_regions limit 1",sep='')
  ), silent=T)
  
  if(length(nrow(exist))==0){
  
          dbSendQuery(ch,
          #cat(
          paste("CREATE TABLE biosmoke_pollution.",poll,"_",stat,"_events_all_regions
          (
            region text,
            date date NOT NULL,
            ",poll,"_",stat," numeric,
            ranked numeric,
            pctile numeric
          )",sep="")
          )
  
  }
  
  rm(exist)
  
  for(town in towns){
  if(town=="Lower Hunter"){
          town='Newcastle'
          }
  try(
  exist<- dbGetQuery(ch,
  #cat(
  paste("select * from biosmoke_pollution.",poll,"_",stat,"_events_",town," limit 1",sep='')
  ), silent=T)
  
  if(length(nrow(exist))>0){
          
          # dbSendQuery(ch,
          # # cat(
          # paste("delete from biosmoke_pollution.",poll,"_",stat,"_events_all_regions where region = \'",town,"\'",sep="")
          # )
  
          dbSendQuery(ch,
          # cat(
          paste("insert into biosmoke_pollution.",poll,"_",stat,"_events_all_regions (region, date, ",poll,"_",stat,", ranked, pctile)
          select '",town,"', date, ",poll,"_",stat,", ranked, pctile
          from  biosmoke_pollution.",poll,"_",stat,"_events_",town,sep="")
          )
  
  }
  rm(exist)
  
  }
  
  }
  
  
#+end_src
**** R stitch_together
#+begin_src R :session *R* :tangle inst/doc/02_loop_over_stations_calculate_net_avg.R :exports none :padline no :eval no
  
  stitch_together(poll="PM10", stat = "av")
  stitch_together(poll="PM25", stat = "av")
  stitch_together(poll="O3", stat = "max")
  dbSendQuery(ch,'grant all on table biosmoke_pollution.pm10_av_events_all_regions to biosmoke_user')   
  dbSendQuery(ch,'grant all on table biosmoke_pollution.pm25_av_events_all_regions to biosmoke_user')
  dbSendQuery(ch,'grant all on table biosmoke_pollution.o3_max_events_all_regions to biosmoke_user')
  
#+end_src

**** COMMENT R main-code
#+name:main
#+begin_src R :session *R* :tangle inst/doc/main.R :exports none :padline no :eval no
  maxdate_selected  <- "2014-12-31"
  source("02_loop_over_stations_calculate_net_avg.R")
  source("03_calc_extreme_events.R")
  # Now Manually validate events
  source("04_qc_checks.R")
  source("05_clean_up_intermediary_tables.R")
#+end_src
**** R clean up intermediary tables
#+begin_src R :session *R* :tangle inst/doc/05_clean_up_intermediary_tables.R :exports none :padline no :eval no
  
  # clean up
  
  dbSendQuery(ch,
  # cat(
  paste("drop table biosmoke_pollution.",'pm10',"_",c('av'),"_events_",gsub('Lower Hunter','Newcastle',towns),"_temp",sep='',collapse=';\n'))
  
  dbSendQuery(ch,
  # cat(
  paste("drop table biosmoke_pollution.",'pm25',"_",c('av'),"_events_",gsub('Lower Hunter','Newcastle',towns),"_temp",sep='',collapse=';\n'))
  
  dbSendQuery(ch,
  # cat(
  paste("drop table biosmoke_pollution.",'o3',"_",c('max'),"_events_",gsub('Lower Hunter','Newcastle',towns[1:4]),"_temp",sep='',collapse=';\n'))
  
  tbls <- pgListTables(ch, "biosmoke_pollution")
  tbls
  # to keep
  "
  4                combined_pollutants biosmoke_pollution
  52         o3_max_events_all_regions biosmoke_pollution
  53        pm10_av_events_all_regions biosmoke_pollution
  54        pm25_av_events_all_regions biosmoke_pollution
  3  pollution_stations_combined_final biosmoke_pollution
  "
  tbls <- read.table(textConnection("rowid                        relname            nspname
  48              imputed_o3_illawarra biosmoke_pollution
  50              imputed_o3_newcastle biosmoke_pollution
  44                  imputed_o3_perth biosmoke_pollution
  46                 imputed_o3_sydney biosmoke_pollution
  16               imputed_pm10_hobart biosmoke_pollution
  11            imputed_pm10_illawarra biosmoke_pollution
  18           imputed_pm10_launceston biosmoke_pollution
  14            imputed_pm10_newcastle biosmoke_pollution
  7                 imputed_pm10_perth biosmoke_pollution
  9                imputed_pm10_sydney biosmoke_pollution
  24               imputed_pm25_hobart biosmoke_pollution
  2             imputed_pm25_illawarra biosmoke_pollution
  26           imputed_pm25_launceston biosmoke_pollution
  6             imputed_pm25_newcastle biosmoke_pollution
  20                imputed_pm25_perth biosmoke_pollution
  22               imputed_pm25_sydney biosmoke_pollution
  49           o3_max_events_illawarra biosmoke_pollution
  51           o3_max_events_newcastle biosmoke_pollution
  45               o3_max_events_perth biosmoke_pollution
  47              o3_max_events_sydney biosmoke_pollution
  17             pm10_av_events_hobart biosmoke_pollution
  12          pm10_av_events_illawarra biosmoke_pollution
  19         pm10_av_events_launceston biosmoke_pollution
  15          pm10_av_events_newcastle biosmoke_pollution
  8               pm10_av_events_perth biosmoke_pollution
  10             pm10_av_events_sydney biosmoke_pollution
  25             pm25_av_events_hobart biosmoke_pollution
  5           pm25_av_events_illawarra biosmoke_pollution
  43         pm25_av_events_launceston biosmoke_pollution
  23          pm25_av_events_newcastle biosmoke_pollution
  21              pm25_av_events_perth biosmoke_pollution
  1              pm25_av_events_sydney biosmoke_pollution
  39          stationdates_hobart_pm10 biosmoke_pollution
  40          stationdates_hobart_pm25 biosmoke_pollution
  35         stationdates_illawarra_o3 biosmoke_pollution
  33       stationdates_illawarra_pm10 biosmoke_pollution
  34       stationdates_illawarra_pm25 biosmoke_pollution
  41      stationdates_launceston_pm10 biosmoke_pollution
  42      stationdates_launceston_pm25 biosmoke_pollution
  38         stationdates_newcastle_o3 biosmoke_pollution
  36       stationdates_newcastle_pm10 biosmoke_pollution
  37       stationdates_newcastle_pm25 biosmoke_pollution
  29             stationdates_perth_o3 biosmoke_pollution
  27           stationdates_perth_pm10 biosmoke_pollution
  28           stationdates_perth_pm25 biosmoke_pollution
  32            stationdates_sydney_o3 biosmoke_pollution
  30          stationdates_sydney_pm10 biosmoke_pollution
  31          stationdates_sydney_pm25 biosmoke_pollution
  "), header = T)
  
  head(tbls)
  
  for(i in 1:nrow(tbls)){
  #i = 1
    dbSendQuery(ch,
  #cat(
  paste("drop table biosmoke_pollution.",tbls$relnam[i],sep='')
    )
  
  }
  
#+end_src

**** R QC
#+begin_src R :session *R* :tangle inst/doc/04_qc_checks.R :exports none :padline no :eval no
  
  # TODO during tests I found there might be duplicated records for some
  # reason so check and rectify if so
  poll <- "o3_max" #"pm10_av" # "pm25_av" # #
  qc <- dbGetQuery(ch,
  paste("SELECT region, date,count(*)
   FROM biosmoke_pollution.",poll,"_events_all_regions
   group by region,date
    having count(*)>1", sep = "")
                   )
  head(qc)
  regiontest <- "Sydney"
  datetest <- "2002-04-07"
  dbGetQuery(ch,
  paste("select *
   FROM biosmoke_pollution.",poll,"_events_all_regions
   where region = '",regiontest,"' and date = '",datetest,"'
  ", sep = "")
  )
  # may have crept in via the station dates process?  
  
  
  
  ############################################################# 
  # summarise  
  
  # TODO: this needs to be looped thru todo rows so the mindate can be selected and missing days counted?
  
  descstats=data.frame(matrix(nrow=0,ncol=15))
  descstats
  for(i in 2:nrow(todo)){
  # i=1
  town=todo[i,1]
  if(town=="Lower Hunter"){
          town='Newcastle'
          } else {
          town=todo[i,1]
          }
  print(town)     
  poll=todo[i,2]
  print(poll)
  
  if(town=="PERTH" & poll=='pm25'){
  mindate=as.factor("'1994-03-01'")
          } else {
  mindate=todo[i,3]
          }
  
  
  
  print(mindate)
  stat=todo[i,4]
  print(stat)
  
  # town=towns[1]
  # print(town)   
          # dbSendQuery(ch,
          # # cat(
          # paste("delete from biosmoke_pollution.",poll,"_",stat,"_events_all_regions where region = \'",town,"\'",sep="")
          # )
  
  d<- dbGetQuery(ch,
          # cat(
          paste("select t1.date as fulldate, t2.*
          from  
          (select distinct date from biosmoke_pollution.stationdates_",town,"_",poll," where date >= ",mindate,") t1 
          left join 
          (select * from biosmoke_pollution.",poll,"_",stat,"_events_all_regions where region =\'",town,"\') as t2
          on t1.date=t2.date",sep="")
          )
          
  counts<- dbGetQuery(ch,
  # cat(
  paste("select \'99\', count(*)
  from
  (
  SELECT region, date, ",poll,"_",stat,", ranked, pctile
    FROM biosmoke_pollution.",poll,"_",stat,"_events_all_regions
    where region = \'",town,"\' and pctile >= .99
    ) foo
  union all
  select \'97-98\', count(*)
  from
  (
  SELECT region, date, ",poll,"_",stat,", ranked, pctile
    FROM biosmoke_pollution.",poll,"_",stat,"_events_all_regions
    where region = \'",town,"\'  and (pctile >= .97 and pctile < .99)
    ) foo
  union all
  select \'95-96\', count(*)
  from
  (
  SELECT region, date, ",poll,"_",stat,", ranked, pctile
    FROM biosmoke_pollution.",poll,"_",stat,"_events_all_regions
    where region = \'",town,"\'  and (pctile >= .95 and pctile < .97)
    ) foo
  union all
  select \'95+\', count(*)
  from
  (
  SELECT region, date, ",poll,"_",stat,", ranked, pctile
    FROM biosmoke_pollution.",poll,"_",stat,"_events_all_regions
    where region = \'",town,"\' and pctile >= .95
    ) foo;",sep="")
  )
          
  head(d)
  descstats=rbind(descstats,
  data.frame(t(c(as.character(town),
          paste(poll,stat),
          nrow(d),
          as.character(min(d$fulldate)),
          as.character(max(d$fulldate)),
          quantile(d[,4],.99,na.rm=T),
          quantile(d[,4],.97,na.rm=T),
          quantile(d[,4],.95,na.rm=T),
          counts[1,2],
          counts[2,2],
          counts[3,2],
          counts[4,2],
          t(
          if (length(names(summary(d[,4])))==6) {
          c(summary(d[,4]),NA)
          } else {
          summary(d[,4])
          }
          ))))
  )
  
  
  }
  
  names(descstats)=c('town','poll','numDays','mindate','maxdate','99','97','95','N99','N97_98','N95_96','N95',names(summary(d[,4])))
  descstats
  #write.csv(descstats,'descstats.csv',row.names=F)
  
  
  
  # I did some manual validation against the original files
  #M:\Environmental_Health\Bushfires\Exposures\TAS
  # etc
  # checked mindates, poll values, even if the single missing days were filled with av of prior and next.
  # for each in todo list.
  # all looks good.
  # only issue was perth mindate for pm2.5 which was no longer cavershamB 15/2/94 but now cavA 1/3/94
    
  # so this caveat is embedded in a if else in the descriptive stats above  
  
  
  #########################################################################################################
  # not changed is the underlying calculation of the percentiles as this would produce trivial changes to the percentile levels.
  ######################################################################################################### 
  
  ######################################################################################################### 
  # NB I did not double check the OZONE values.
  
  # useful code
  # select t1.date as fulldate, t2.*
  # from  
  # (select distinct date from biosmoke_pollution.stationdates_Sydney_pm10 where date >= '1994-01-10') t1 
  # left join 
  # (select * from biosmoke_pollution.pm10_av_events_all_regions where region ='Newcastle') as t2
  # on t1.date=t2.date
  
  
  # select *  
  # from  
  # (select distinct date from biosmoke_pollution.stationdates_illawarra_pm25 where date = '1998-03-01') t1 
  # left join 
  # (
  # select biosmoke_pollution.combined_pollutants.* 
  # from biosmoke_pollution.combined_pollutants 
  # join 
  # spatial.pollution_stations_combined_final
  # on
  # biosmoke_pollution.combined_pollutants.site=spatial.pollution_stations_combined_final.site 
  # where region = 'Illawara'
  # ) t2
  # on t1.date=t2.date
    
    
  
#+end_src
*** step4
#+begin_src R :session *R* :tangle inst/doc/methods.Rmd :exports none :eval no :padline yes 
  
  Step 4. Validate events and identify the causes. Select any events
  with PM10 or PM2.5 greater than 95 percentile. Manually validate
  events using online newspaper archives, government and research agency
  records, satellite imagery and other sources (such as a Dust Storm
  database).  Enter the information for each event into the custom built
  data entry forms.  For any events with references for multiple types
  of source, assess the liklihood of any single source being the
  dominant source.  Double check any remaining 99th percentile dates with no
  references.
  
#+end_src
**** TODO 03_calc_extreme_events.R
*** COMMENT 03_calc_extreme_events.R-code
#+name:03_calc_extreme_events.R
#+begin_src R :session *R* :tangle inst/doc/03_calc_extreme_events.R :exports none :eval no
  #### name:03_calc_extreme_events.R ####
  # now make a view for each poll so we can see what has been checked and what still needs to be checked
  
  for(poll in c("pm10_av", "pm25_av", "o3_max")){
  #poll = "pm10_av"
  txt <-  paste("
  create or replace view biosmoke_events.",poll,"_checked
  as 
  select ",poll,".region, ",poll,".date, cast(",poll,".pctile*100 as integer) as pctile, refid, eventid
  from
  biosmoke_pollution.",poll,"_events_all_regions as ",poll,"
  left join
  (
          SELECT t1.date, t2.*
          FROM 
                  biosmoke_pollution.",poll,"_events_all_regions t1
          ,
                  (
                  select tab1.*, 
                  case when place like 'Sydney%' then 'Sydney' else place end as region,
                  field3,field5, field7 from
                  biosmoke_events.tblevents tab1
                  join biosmoke_events.tblreferences tab2
                  on tab1.refid=tab2.refid
                  ) t2
          where t1.region=t2.region and 
                  (
                  t1.date=t2.mindate 
                  or
                  (t1.date >= t2.mindate and t1.date <= t2.maxdate)
                  )
  ) checked
  on ",poll,".date=checked.date
  and ",poll,".region=checked.region 
  where pctile>=.95 and mindate is not null 
    ORDER BY ",poll,".region, ",poll,".pctile DESC;
  grant select on biosmoke_events.",poll,"_checked to biosmoke_user;
  
  create or replace view biosmoke_events.",poll,"_to_check
  as 
  select ",poll,".region, ",poll,".date, cast(",poll,".pctile*100 as integer) as pctile, refid, eventid
  from
  biosmoke_pollution.",poll,"_events_all_regions as ",poll,"
  left join
  (
          SELECT t1.date, t2.*
          FROM 
                  biosmoke_pollution.",poll,"_events_all_regions t1
          ,
                  (
                  select tab1.*, 
                  case when place like 'Sydney%' then 'Sydney' else place end as region,
                  field3,field5, field7 from
                  biosmoke_events.tblevents tab1
                  join biosmoke_events.tblreferences tab2
                  on tab1.refid=tab2.refid
                  ) t2
          where t1.region=t2.region and 
                  (
                  t1.date=t2.mindate 
                  or
                  (t1.date >= t2.mindate and t1.date <= t2.maxdate)
                  )
  ) checked
  on ",poll,".date=checked.date
  and ",poll,".region=checked.region 
  where pctile>=.95 and mindate is null 
    ORDER BY ",poll,".region, ",poll,".pctile DESC;
  grant select on biosmoke_events.",poll,"_to_check to biosmoke_user
  ",sep="")
  
  cat(txt)
  dbSendQuery(ch, txt)
  }
#+end_src

**** COMMENT func QC missing99
#+name:func sites_todo
#+begin_src R :session *R* :tangle R/missing99.R :exports none :padline no :eval yes
  #' @name   missing99
  #' @title   99th centile missing references of any type
  #' @param poll pollutant
  #' @return list of dates
  missing99 <- function(poll){
  dat <- dbSendQuery(ch,
  # cat(
  paste("
  create or replace view biosmoke_pollution.",poll,"_to_check
  as 
  select ",poll,".*, eventid,refid, eventtype, place,mindate,maxdate, field3,field5, field7
  from
  biosmoke_pollution.",poll,"_av_events_all_regions as ",poll,"
  left join
  (
          SELECT t1.date, t2.*
          FROM 
                  biosmoke_pollution.",poll,"_",stat,"_events_all_regions t1
          ,
                  (
                  select tab1.*, 
                  case when place like 'Sydney%' then 'Sydney' else place end as region,
                  field3,field5, field7 from
                  ivan_hanigan.tblevents tab1
                  join ivan_hanigan.tblreferences tab2
                  on tab1.refid=tab2.refid
                  ) t2
          where t1.region=t2.region and 
                  (
                  t1.date=t2.mindate 
                  or
                  (t1.date >= t2.mindate and t1.date <= t2.maxdate)
                  )
  ) checked
  on ",poll,".date=checked.date
  and ",poll,".region=checked.region 
  where pctile>=.99 and mindate is null 
    ORDER BY ",poll,".region, ",poll,".pctile DESC;
  grant all on biosmoke_pollution.",poll,"_to_check to biosmoke_group
  ",sep="")
  )
  return(dat)
  }
  
  
  
#+end_src
**** R QC missing99
#+begin_src R :session *R* :tangle inst/doc/04_qc_checks.R :exports none :padline no :eval no
  # identify 99% centile days with no refs.
  missing99(poll=polls[5,3])
  missing99(poll=polls[7,3])

#+end_src
** QC graphs and tables
orig joining of events and pollution was from work PC
I:/projects/1.302 Biomass/analysis/exposures/event validation/impute/quality_control/do.r


head/projects/Biomass/data/pollution/summaryValidPollution4events
I:\Dropbox\projects\1.302 Biomass\data\pollution\summaryValidPollution4events
*** COMMENT TODO qc-plot-code
#+name:qc-plot
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:qc-plot ####
  library(rpostgrestools)
  ch <- connect2postgres2("ewedb_staging")
  
  poll  <-  "pm25"
  sites <- "'Richmond', 'Liverpool', 'Earlwood', 'Chullora'"
  qc <- dbGetQuery(ch,
  sprintf("
  SELECT site, date, rawdata, imputed_param
    FROM biosmoke_pollution.imputed_%s_sydney
  where site in (%s)
  order by date
  ", poll, sites)
                   )
  head(qc)
  tail(qc)
  with(qc, plot(date, imputed_param, type = "l", ylim = c(0,200), col = 'grey'))
  with(na.omit(qc), lines(lowess(imputed_param ~ date, f=.02)))
  
  # looks like pm2.5 changedFiles
  SELECT t1.site, min(date)
    FROM biosmoke_pollution.combined_pollutants t1
    join biosmoke_pollution.pollution_stations_combined_final t2
    on t1.site = t2.site 
    where pm25_av is not null and region = 'Sydney'
    group by t1.site
    order by min(date);
  
  SELECT site, date, rawdata, imputed_param
    FROM biosmoke_pollution.imputed_pm25_sydney
    where site = 'Richmond'
    order by date;
  
  
  SELECT site, date, pm25_av
    FROM biosmoke_pollution.combined_pollutants
    where (site = 'Richmond' or site = 'Earlwood')
    and date between '2011-07-01' and '2011-07-29'
    order by date;
  
#+end_src

*** COMMENT qc-pm25-output-code
#+name:qc-pm25-output
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:qc-pm25-output ####
  library(rpostgrestools)
  ch <- connect2postgres2("ewedb_staging")
  
  poll  <-  "pm25"
  sites <- "'Richmond', 'Liverpool', 'Earlwood', 'Chullora'"
  
  dbGetQuery(ch,
  "SELECT site, date, rawdata, date, networkavg, missingavg3mo, networkavg3mo, 
         imputed, imputed_param
    FROM biosmoke_pollution.imputed_pm25_sydney
    where site = 'Liverpool' and date between '2004-01-01' and '2004-02-17'
    order by date
  "
             )
  
  
  
  
  
  
  qc <- dbGetQuery(ch,
  sprintf("
  SELECT site, date, rawdata, imputed_param
    FROM biosmoke_pollution.imputed_%s_sydney
  where site in (%s)
  order by date
  ", poll, sites)
                   )
  head(qc)
  tail(qc)
  with(qc, plot(date, imputed_param, type = "l", ylim = c(0,200), col = 'grey'))
  with(na.omit(qc), lines(lowess(imputed_param ~ date, f=.02)))
  
#+end_src

** availability
#+begin_src R :session *R* :tangle inst/doc/methods.Rmd :exports none :padline no :eval no :padline yes 
  
  ## Availability and requirements
  
  Lists the following:
  
  - Project name: BiosmokeValidatedEvents
  - Project home page: http://swish-climate-impact-assessment.github.io/BiosmokeValidatedEvents/
  - Operating system(s): R package is platform independent.  Data Entry forms are Microsoft Windows.
  - Programming language: R and SQL
  - Other requirements: PostgreSQL (PostGIS is desirable)
  - License: CC BY 4.0
  - Any restrictions to use: amendments of errors of ommision or commission are invited but will be vetted before insertion into the master database.
  
  ## Availability of supporting data
  
  BMC Research Notes encourages authors to deposit the data set(s) supporting the results reported in submitted manuscripts in a publicly-accessible data repository, when it is not possible to publish them as additional files. This section should only be included when supporting data are available and must include the name of the repository and the permanent identifier or accession number and persistent hyperlink(s) for the data set(s). The following format is required:
  
  "The data set(s) supporting the results of this article is(are) available in the [repository name] repository, [unique persistent identifier and hyperlink to dataset(s) in http:// format]."
  
  Where all supporting data are included in the article or additional files the following format is required:
  
  "The data set(s) supporting the results of this article is(are) included within the article (and its additional file(s))"
  
  We also recommend that the data set(s) be cited, where appropriate in the manuscript, and included in the reference list.
  
  A list of available scientific research data repositories can be found here. A list of all BioMed Central journals that require or encourage this section to be included in research articles can be found here.
#+end_src
** COMMENT bib-code
#+name:bib
#+begin_src R :session *R* :tangle inst/doc/methods.Rmd :exports none :padline no :eval no

  **References**

  ```{r, echo=FALSE, message=FALSE, eval = T}
  write.bibtex(file="references.bib")
  ```
  
#+end_src

** ETL metadata 
*** COMMENT go-code
#+name:go
#+begin_src R :session *R* :tangle no :exports none :padline no :eval yes
  #### name:go ####
  require(rpostgrestools)
  ch <- connect2postgres2("data_inventory_hanigan_dev4")
  setwd("~/data/BiosmokeValidatedEvents/inst/doc/")
  projs <- dbGetQuery(ch, "select id, title from project order by id")
  projs
  #for(i in 1:nrow(projs)){
  i = 2
    project = projs[i,2]
  project
  dsets <- dbGetQuery(ch,
                      sprintf("select shortname from dataset where project_id = %s order by id", projs[i,1])
                      )
  #for(dataset in dsets){
  dsets[,1]
  dataset = dsets[1,1]
  dataset  
  #}
  
  #}
  
  library(rmarkdown)
  library(knitr)
  
  dir()
  #render("data_deposit_form.Rmd") 
  knitr::knit2html("data_deposit_form.Rmd", stylesheet='custom.css')
  #browseURL("data_deposit_form.html")
  # no good, do in word? system("pandoc -i data_deposit_form.html -o data_deposit_form.docx")
#+end_src

#+RESULTS: go
: data_deposit_form.html

*** summary of data for ETL
#+begin_src R :session *R* :tangle inst/doc/data_deposit_form.Rmd :exports none :eval no :padline no
  ---
  title: DDF
  output: html_document
  ---
  
  # Introduction
    
  # Project level information
  
  ```{r, echo = F, eval = T, results="hide"}
  #### name:summary of project info ####
  if(exists('ch'))   dbDisconnect(ch)
  library(swishdbtools)
  library(sqldf)
  library(knitr)
  library(xtable)  
  
  ch  <- connect2postgres2("data_inventory_hanigan_dev4")
  prj <- project
  dset <- dataset
  
  ```
  ```{r, echo = F, eval = T, results="asis"}  
  
  dat <- sqldf(connection = ch,
    sprintf("select t1.*
    from project t1
    where t1.title = '%s'", prj)
    )
  #names(dat)
  #t(dat)
  
  ####  help
  help  <- sqldf(connection = ch,
    "select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%project%'"
    )
  # head(help)
  
  dat_i <- data.frame(V1 = names(dat), V2=t(dat[1,]))
  #dat_i
  dat_i$order <- 1:nrow(dat_i)  
  qc <- merge(dat_i, help, by.x = "V1", by.y = "datinv", all.x = TRUE)
  qc2 <- qc[order(qc$order),c(1,2,5)]
  names(qc2) <- c("variable", "value", "help_comment")
  
  qc2[,2] <- gsub("\n", " | ", qc2[,2])
  print(xtable(qc2), type = "html", include.rownames = F)
  
  ```
  
  # Dataset level information (data packages)
  ```{r, echo = F, eval = T, results="asis"}
  
  #### for each dataset
  #dat$shortname
  # for(i in 1:nrow(dat)){
  
  # i = which(dat$shortname == dset)
  
  
  dat <- dbGetQuery(ch,
  sprintf("select * from dataset
   where shortname ='%s'", dset)
  )
  dat_i <- data.frame(V1 = names(dat), V2=t(dat[1,]))
  # dat_i
   dat_i$order <- 1:nrow(dat_i)
    #title <- paste(c(as.character(dat_i[dat_i$V1 %in% c('shortname','title'),2])),
    #      collapse = ", ", sep = "")
    #title
  
  help  <- dbGetQuery(ch,
    "select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%dataset%'"
    )
  # head(help)
  
  
  qc <- merge(dat_i, help, by.x = "V1", by.y = "datinv", all.x = TRUE)
  #qc[1,]
  #names(qc)
  qc2 <- qc[order(qc$order),c(1,2,5)]
  #qc2
  #qc3 <- data.frame(index1 = rep(paste("0. dataset", dset), nrow(qc2)),
  #           index2 = c(title, rep("", nrow(qc2) - 1)),
  #           metadata = qc2)
  #names(qc3) <- c("index1", "index2", "variable", "value", "help")
  names(qc2) <- c("variable", "value", "help")
  #names(qc2)
  #### Keyword
  ky <- dbGetQuery(ch,
    #cat(q
    paste("select t3.keyword
    from dataset t1
    join keyword t3
    on t1.id = t3.dataset_id
    where t1.shortname = '",dset,"'
    ", sep = "")
  )
  
  if(nrow(ky) > 0){
  ky <- ky[,1]
  } else {
  ky <- ''
  }
  ky <- paste(ky, sep = "", collapse=", ")
  ky <- data.frame(variable = "keywords", value = ky, help="Keywords or phrases that concisely describe the resource. Example is biodiversity. Use a controlled vocabulary thesaurus")
  
  
  qc_out <- rbind(qc2, ky)
  #qc_out[,1:3]
  #qc_out
  
  #kable(qc_out, row.names = F)
  
  
  dat <- dbGetQuery(ch,
  sprintf("select t1.*
  from intellectualright t1
  join dataset t2
  on t1.dataset_id =  t2.id
   where shortname ='%s'", dset)
  )
  if(nrow(dat) == 0){
  dat <- data.frame(id = '', dataset_id = '', licence_code = '',
    licence_text = '', special_conditions='')
  }
  dat_i <- data.frame(V1 = names(dat), V2=t(dat[1,]))
   #dat_i
   dat_i$order <- 1:nrow(dat_i)
    #title <- paste(c(as.character(dat_i[dat_i$V1 %in% c('shortname','title'),2])),
    #      collapse = ", ", sep = "")
    #title
  
  help  <- sqldf(connection = ch,
    "select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%intellectualright%'"
    )
  # head(help)
  qc <- merge(dat_i, help, by.x = "V1", by.y = "datinv", all.x = TRUE)
  qc2 <- qc[order(qc$order),c(1,2,5)]
  names(qc2) <- c("variable", "value", "help")
  # names(  qc2)
  qc_out <- rbind(qc_out, qc2[-c(1,2),])
  qc_out[,2] <- gsub("\n", " | ", qc_out[,2])
  print(xtable(qc_out), type = "html", include.rownames = F)
  ```
  
  # Entity level information (files)  
  
  ```{r, echo = F, eval = T, results="asis"}
    
  #### entity ####
  #dat <- dbGetQuery(ch, "select * from entity")
  dat_ent <- dbGetQuery(ch,
  #cat(
  sprintf("select 
  t3.*
  from project t1
  join dataset t2
  on t1.id = t2.project_id
  join entity t3
  on t2.id = t3.dataset_id
  where t1.title = '%s'
  and t2.shortname = '%s'", prj, dset),
  )
  # head(dat_ent)
  
  help_ent  <- sqldf("select t1.eml_node, t1.help_comment, t1.datinv
    from crosswalk t1
    where eml_table like '%entity%'",
    connection = ch)
  #help_ent
  
  
  for(j in 1:nrow(dat_ent)){
  #j = 1
  print(paste("#### File", j))
  ent_j <- data.frame(V1 = names(dat_ent), V2=t(dat_ent[j,]))
  ent_j$order <- 1:nrow(ent_j)
  #title2 <- paste(c(j, "entity", as.character(ent_j[1,2])),
  #        collapse = ", ", sep = "")
  #  title2
    qc_ent <- merge(ent_j, help_ent, by.x = "V1", by.y = "datinv", all.x = T)
    qc_ent2 <- qc_ent[order(qc_ent$order),c(1,2,5)]
  #qc_ent2
  #qc_ent3 <- data.frame(index = rep(title2, nrow(qc_ent2)),
  #                      index = c(title2, rep("", nrow(qc_ent2) - 1)),
  #                      meta = qc_ent2)
  names(qc_ent2) <- c("variable","value","help_comment")
  qc_ent2[,2] <- gsub("\n", " | ", qc_ent2[,2])
  #print(kable(qc_ent2, row.names = F))
  
  print(xtable(qc_ent2), type = "html", include.rownames = F)
  #write.csv(qc_ent2, paste(dset, "_data_deposit_form.csv", sep = ""), row.names = F)
  }
  
  
    
  ```
  
#+end_src

#+RESULTS:


** COMMENT conceptual-diagram-code
#+name:conceptual-diagram
#+begin_src R :session *R* :tangle inst/doc/conceptual-diagram.R :exports none :padline no :eval yes
  #### name:conceptual-diagram ####
  setwd("~/data/BiosmokeValidatedEvents/inst/doc")
  library(disentangle)
  library(stringr)
  dat <- read.csv("conceptual-diagram.csv", stringsAsFactor = F)
  dat <- dat[dat$DONTSHOW != "Y", ]
  summary(dat)
  flowchart <- newnode_df(
    indat = dat
    ,
    names_col = "name"
    ,
    in_col = "inputs"
    ,
    out_col = "outputs"
    ,
    clusters_col= "group"
    ,
    desc_col="description"
    )
  
  sink("fileTransformations.dot")
  cat(flowchart)
  sink()
  system("dot -Tpdf fileTransformations.dot -o fileTransformations.pdf")
  
#+end_src

#+RESULTS: conceptual-diagram
: 0

** COMMENT reference review
media/external/u3171954-H/My Documents/projects/1.302 Biomass/analysis/exposures/event validation/Archive_20100609/REFS

** COMMENT read  Methods back from word and insert to data inventory using sql???
*** COMMENT methods-code
#+name:methods
#+begin_src R :session *R* :tangle no :exports none :padline no :eval yes
  #### name:methods ####
  if(exists('ch'))   dbDisconnect(ch)
  etl <- "load"
  library(rpostgrestools)
  ch <- connect2postgres2("data_inventory_hanigan_dev4")
  setwd("~/data/bio_validated_bushfire_events")
  dir()
  dset <- "bio_validated_bushfire_events"
  
  pid <- dbGetQuery(ch,
  #cat(                  
  sprintf("select project_id
  from dataset
  where shortname = '%s'",
                    dset
                    )
  )
  pid
  
  prj <- dbGetQuery(ch,
  sprintf("select *
  from project
  where id = %s",
                    pid
             )
  )
  prj <- as.matrix(t(prj))
  if(etl == "extract"){
  write.csv(prj, "project.csv", row.names=T)
  } 
  #### edit this ####
  prj  <- read.csv("project.csv", stringsAsFactor = F)
  prj 
  prj <- prj[-which(prj[,2] == ''),]
  input <- prj[,2]
  nums <- as.numeric(input)
  
  replace  <-   which(is.na(nums))
  dont_replace  <-  which(!is.na(nums))
  
  rplace <- gsub("NA", "", paste("'", paste(input[replace], "'", sep = ""), sep = ""))
  rplace_df <- as.data.frame(rbind(
  cbind(dont_replace, input[dont_replace])
        ,
  cbind(replace, rplace)
  ))
  
  rplace_df <- cbind(rplace_df, prj[,1])
  txt <- paste(apply(rplace_df[,3:2], 1, paste, collapse = " = "), sep = "", collapse = ", ")
  cat(txt)
  # TODO don;t do empty strings  
  dbSendQuery(ch,
  #cat(            
  sprintf("UPDATE project
     SET %s
   WHERE id = %s",  txt, pid)
  )
  
  ## UPDATE project
  ##    SET id=?, title=?, abstract=?, studyareadescription=?, personnel=?, 
  ##        funding=?, personnel_owner_organisationname=?, personnel_data_owner=?
  ##  WHERE <condition>;
  
  
  ## dbSendQuery(ch, "UPDATE dataset
  ## SET method_steps='
  ## Step 1: acquire the smoke pollution data from State Governments.
  ## Step 2: load into a postgres database.
  
  ## See /media/Seagate Expansion Drive/u3171954-H/My Documents/projects/1.302 Biomass/analysis/exposures/event validation/impute
  ## which I need to compare with
  ## /media/Seagate Expansion Drive/ivan_acer/projects/1.302 Biomass/analysis/exposures/event validation/versions/2012-01-12/impute
  
  ## '
  ## WHERE shortname = 'bio_validated_bushfire_events';
  ## ")
  
#+end_src

#+RESULTS: methods

* COMMENT get-data-delphe-code
** get-data-delphe
#+begin_src R :session *R* :tangle no :exports none :padline no :eval no
  ################################################################
  # name:get-data-delphe
  require(rpostgrestools)
  setwd("~/projects/biomass_smoke_and_human_health/BiosmokeValidatedEvents/inst/extdata")
  ch <- connect2postgres2("delphe")
  
  tbls <- c("bio_events.tblreferences",
  "bio_events.tblevents",
  "bio_events.dust_event_records",
  "bio_events.dust_event_records2")
  dir()
  for(tb in tbls)
    {
      #tb  <- tbls[1]
      print(tb)
      df <- sql_subset(ch, tb, eval = T)
      #str(df)
      write.csv(df, paste(tb, ".csv", sep = ""), row.names = FALSE, na = "")
    }
  
#+end_src

** COMMENT load-data-ewedb_staging-code
#+name:load-data-ewedb
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:load-data-ewedb ####
  require(rpostgrestools)
  setwd("~/projects/biomass_smoke_and_human_health/BiosmokeValidatedEvents/inst/extdata")
  dir()
  ch <- connect2postgres2("ewedb_staging")
  
  #### set up, actually did on pgadmin as postgres
  #dbSendQuery(ch,
  "
  
  CREATE TABLE biosmoke_events.tblreferences
  (
    refid serial NOT NULL,
    field1 text,
    field2 text,
    field3 text,
    field4 integer NOT NULL,
    field5 text,
    field6 text,
    field7 text NOT NULL,
    field8 text,
    field9 text,
    field10 integer,
    field11 text,
    field12 text,
    field13 text,
    field14 text,
    field15 text,
    field16 text,
    field17 text,
    field18 text,
    field19 text,
    field20 text,
    field21 text,
    field22 text,
    field23 text,
    field24 text,
    field25 text,
    field26 text,
    field27 text,
    field28 text,
    field29 text,
    field30 text,
    field31 text,
    field32 text,
    field33 text,
    field34 text,
    field35 text,
    field36 text,
    field37 text,
    field38 text,
    field39 text,
    field40 text,
    field41 text,
    CONSTRAINT biosmoke_events_tblreferences_pkey PRIMARY KEY (refid),
    CONSTRAINT biosmoke_events_credential_check CHECK (field28 = 'toms'::text OR field28 = 'government'::text OR field28 = 'journal'::text OR field28 = 'media'::text OR field28 = 'modis smoke'::text OR field28 = 'modis hotspot'::text OR field28 = 'internet'::text)
  )
  WITH (
    OIDS=FALSE
  );
  ALTER TABLE biosmoke_events.tblreferences
    OWNER TO postgres;
  GRANT ALL ON TABLE biosmoke_events.tblreferences TO ivan_hanigan;
  GRANT select ON TABLE biosmoke_events.tblreferences TO biosmoke_user;
  GRANT ALL ON sequence biosmoke_events.tblreferences_refid_seq TO ivan_hanigan;
  
  CREATE TABLE biosmoke_events.tblevents
  (
    eventid serial NOT NULL ,
    refid integer,
    eventid2 integer,
    eventtype character varying(255),
    place character varying(255) NOT NULL,
    mindate date NOT NULL,
    maxdate date,
    burnareaha character varying(255),
    metconditions character varying(255),
    CONSTRAINT biosmoke_events_tblevents_pkey PRIMARY KEY (eventid),
    CONSTRAINT biosmoke_events_tblref_cscd FOREIGN KEY (refid)
        REFERENCES biosmoke_events.tblreferences (refid) MATCH SIMPLE
        ON UPDATE CASCADE ON DELETE CASCADE,
    CONSTRAINT biosmoke_events_eventtype_check CHECK (eventtype::text = 'bushfire'::text OR eventtype::text = 'dust'::text OR eventtype::text = 'salt'::text OR eventtype::text = 'possible biomass'::text OR eventtype::text = 'prescribed burn'::text OR eventtype::text = 'woodsmoke'::text OR eventtype::text = 'non-biomass, fire'::text OR eventtype::text = 'non-biomass, non-fire'::text)
  )
  WITH (
    OIDS=FALSE
  );
  ALTER TABLE biosmoke_events.tblevents
    OWNER TO postgres;
  GRANT ALL ON TABLE biosmoke_events.tblevents TO ivan_hanigan;
  GRANT select ON TABLE biosmoke_events.tblevents TO biosmoke_user;
  GRANT ALL ON sequence biosmoke_events.tblevents_eventid_seq TO ivan_hanigan;
  
  "
  #            )
  
  
  
  
  #### load
  tbls <- c("bio_events.tblreferences",
  "bio_events.tblevents",
  "bio_events.dust_event_records",
  "bio_events.dust_event_records2")
  dir()
  #for(tb in tbls)
  #  {
  tb  <- tbls[2]
  print(tb)
   
  tbin <-    read.csv(paste(tb, ".csv", sep = ""), stringsAsFactor = FALSE)
      str(tbin)
  tbout <- gsub("bio_events.", "", tb)
  tbouttmp <- gsub("bio_events.", "temp", tb)
      tbouttmp
      dbWriteTable(ch, tbouttmp, tbin, row.names = F)
  
  
  #    dbSendQuery(ch, sprintf("insert into biosmoke_events.%s ()",
  #    tbout, tbouttmp))
  # done manually in pgadmin!
  
  dbRemoveTable(ch, tbouttmp)
  '
  # NB some break constraint of not null field28
  169;"IT Cental University of Tasmania";"";"http://home.iprimus.com.au/foo7/firesnsw.html"
  166;"IT Cental University of Tasmania";"";"http://home.iprimus.com.au/foo7/firesnsw.html"
  167;"IT Cental University of Tasmania";"";"http://home.iprimus.com.au/foo7/firesnsw.html"
  168;"IT Cental University of Tasmania";"";"http://home.iprimus.com.au/foo7/firesnsw.html"
  165;"IT Cental University of Tasmania";"";"http://home.iprimus.com.au/foo7/firesnsw.html"
  642;"bljkhg";"";""
  179;"aap australia general news";"";"http://www.highbeam.com/doc/1P1-104619075.html"
  180;"aap australian general news";"";"http://www.highbeam.com/doc/1P1-104619075.html"
  786;"EMA Disasters Database";"";"http://www.ema.gov.au/ema/emadisasters.nsf/54273a46a9c753b3ca256d0900180220/20b65e3cc204d21aca256d3300057db6?OpenDocument&Highlight=0,perth,bushfire,1997"
  
  So added as best could, added a value for internet, aap is media
  '
  #}
  
#+end_src

  
#+end_src
** COMMENT load-data-ewedb_prod-code
#+name:load-data-ewedb
#+begin_src R :session *R* :tangle no :exports none :eval no
# make sure remove server has schema and users as appropriate
pg_dump -h localhost -p 5432 -U postgres -i -n \"biosmoke_spatial\" ewedb_staging | psql -h gislibrary -U postgres ewedb
pg_dump -h localhost -p 5432 -U postgres -i -n \"biosmoke_pollution\" ewedb_staging | psql -h gislibrary -U postgres ewedb
pg_dump -h localhost -p 5432 -U postgres -i -n \"biosmoke_events\" ewedb_staging | psql -h gislibrary -U postgres ewedb


#+end_src
* COMMENT BMC data note 
http://www.biomedcentral.com/bmcresnotes/authors/instructions/datanote

*** COMMENT latex_head-code
run with R studio
#+name:latex_head
#+begin_src sh :session *shell* :tangle vignettes/BiosmokeValidatedEvents_DataNote.tex :exports none :eval no :padline no
%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% http://www.biomedcentral.com/info/authors/                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% http://www.miktex.org                                           %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins

%%% loading packages, author definitions

%\documentclass[twocolumn]{bmcart}% uncomment this for twocolumn layout and comment line below
\documentclass{bmcart}

%%% Load packages
%\usepackage{amsthm,amsmath}
%\RequirePackage{natbib}
%\RequirePackage[authoryear]{natbib}% uncomment this for author-year bibliography
%\RequirePackage{hyperref}
\usepackage[utf8]{inputenc} %unicode support
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\def\includegraphic{}
\def\includegraphics{}



%%% Put your definitions there:
\startlocaldefs
\endlocaldefs


%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Data Note (Draft 2015-07-14)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{An Online Extensible Database of Validated Extreme Air Pollution Events for Health Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
   addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
   corref={aff1},                       % id of corresponding address, if any
%   noteref={n1},                        % id's of article notes, if any
   email={ivan.hanigan@anu.edu.au}   % email address
]{\inits{IC}\fnm{Ivan C} \snm{Hanigan}}
\author[
   addressref={aff2},
   email={fay.johnston@utas.edu.au}
]{\inits{FH}\fnm{Fay H} \snm{Johnston}}
\author[
   addressref={aff3},
   email={geoff.morgan@nsw.gov.au}
]{\inits{GG}\fnm{Geoff G} \snm{Morgan}}
\author[
   addressref={aff2},
   email={someone@somewhere.com.au}
]{\inits{SS}\fnm{Someone} \snm{Else}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{%                           % unique id
  \orgname{National Centre of Epidemiology, ANU}, % university, etc
  \street{Eggleston Road},                     %
  %\postcode{}                                % post or zip code
  \city{Canberra},                              % city
  \cny{AU}                                    % country
}
\address[id=aff2]{%
  \orgname{Menzies School, UTAS},
  \street{D\"{u}sternbrooker Weg 20},
  \postcode{}
  \city{Hobart},
  \cny{AU}
}
\address[id=aff3]{%
  \orgname{University of Sydney, NSW},
  \street{D\"{u}sternbrooker Weg 20},
  \postcode{}
  \city{Sydney},
  \cny{AU}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{artnotes}
%\note{Sample of title note}     % note to the article
%\note[id=n1]{Equal contributor} % note, connected to author
%\end{artnotes}

\end{fmbox}% comment this for two column layout
#+end_src
*** abstract-methods
#+begin_src sh :session *shell* :tangle vignettes/BiosmokeValidatedEvents_DataNote.tex :exports none :eval no :padline no
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract} % abstract
\parttitle{Background} %if any
Epidemiological studies of the health effects of
biomass smoke events (such as bushfires or wood-heater smoke spikes due
to inversion layers) have been hampered by the availability of datasets
that explicitly pertain to these sources. Extreme air pollution events
may also be caused by dust storms, fossil fuel induced smog events or
factory fires. This paper presents an open and extensible database
developed by the authors to identify historical spikes in PM
concentrations and to evaluate whether they were caused by vegetation
fire smoke or by other possible sources. These methods provide a
systematic framework for retrospective identification of the air quality
impacts of biomass smoke in a region that is seasonally affected by
fires. In this paper, we describe the database and data aquisition
methods, as well as analytical considerations when validating historical
events using a range of reference types.

\parttitle{Methods} %if any
 Several major urban centers and smaller regional towns
in the Australian states of New South Wales, Western Australia, and
Tasmania were selected as they are intermittently affected by extreme
episodes of vegetation fire smoke. Air pollution data was collated and
missing values were imputed. Extreme values were identified and a range
of sources of reference information were assessed for each date.
Reference types online newspaper archives, government and research
agency records, satellite imagery and a Dust Storms database.

\parttitle{Results}
This dataset contains validated events of extreme
biomass smoke pollution across Australian cities. The authors have
previously demonstrated the utility of this database in analyses of
hospital admissions and mortality data for these locations to quantify
the pollution-related health effects of these events.

\parttitle{Conclusions}
The database was created using open source
software and this makes the prospect for future extensions to the
database possible. THis is because if other scientists notice an
ommision or error in these data they can offer an amendment. We believe
that this will improve the database and benefit the whole biomass smoke
health research community.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{sample}
\kwd{article}
\kwd{author}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
%\end{fmbox}% uncomment this for twcolumn layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Main Body begins here                %%
%%                                          %%
%% Please refer to the instructions for     %%
%% authors on:                              %%
%% http://www.biomedcentral.com/info/authors%%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%% See the Results and Discussion section   %%
%% for details on how to create sub-sections%%
%%                                          %%
%% use \cite{...} to cite references        %%
%%  \cite{koon} and                         %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%

%\section*{Background}

%Following the other paper.

\section*{Findings}

\input{methods}

The LaTeX template needs bibtex style citations, so here is one to ensure the compiler works while creating drafts.  The main paper to cite is \cite{Johnston2011c}.

\section*{Instructions for Accessing the Database}

The Database can be accessed by the pgAdmin tool for PostgreSQL databases, the R software or by using ODBC and MS Access.  The latter method is the recommended way to view the data entries using Forms stored in the MS Access database provided with the downloadable materials.  A Password is available on request to the corresponding author.

An additional document shows the instructions to access the database in more detail [see Additional file 1].

#+end_src
*** snip
#+begin_src sh :session *shell* :tangle no :exports none :eval no :padline no

%\section*{Content}
%Text and results for this section, as per the individual journal's instructions for authors. %\cite{koon,oreg,khar,zvai,xjon,schn,pond,smith,marg,hunn,advi,koha,mouse}

%\section*{Section title}
Text for this section \ldots
\subsection*{Sub-heading for section}
Text for this sub-heading \ldots
\subsubsection*{Sub-sub heading for section}
Text for this sub-sub-heading \ldots
\paragraph*{Sub-sub-sub heading for section}
Text for this sub-sub-sub-heading \ldots
In this section we examine the growth rate of the mean of $Z_0$, $Z_1$ and $Z_2$. In
addition, we examine a common modeling assumption and note the
importance of considering the tails of the extinction time $T_x$ in
studies of escape dynamics.
We will first consider the expected resistant population at $vT_x$ for
some $v>0$, (and temporarily assume $\alpha=0$)
%
\[
 E \bigl[Z_1(vT_x) \bigr]= E
\biggl[\mu T_x\int_0^{v\wedge
1}Z_0(uT_x)
\exp \bigl(\lambda_1T_x(v-u) \bigr)\,du \biggr].
\]
%
If we assume that sensitive cells follow a deterministic decay
$Z_0(t)=xe^{\lambda_0 t}$ and approximate their extinction time as
$T_x\approx-\frac{1}{\lambda_0}\log x$, then we can heuristically
estimate the expected value as
%
\begin{eqnarray}\label{eqexpmuts}
E\bigl[Z_1(vT_x)\bigr] &=& \frac{\mu}{r}\log x
\int_0^{v\wedge1}x^{1-u}x^{({\lambda_1}/{r})(v-u)}\,du
\nonumber\\
&=& \frac{\mu}{r}x^{1-{\lambda_1}/{\lambda_0}v}\log x\int_0^{v\wedge
1}x^{-u(1+{\lambda_1}/{r})}\,du
\nonumber\\
&=& \frac{\mu}{\lambda_1-\lambda_0}x^{1+{\lambda_1}/{r}v} \biggl(1-\exp \biggl[-(v\wedge1) \biggl(1+
\frac{\lambda_1}{r}\biggr)\log x \biggr] \biggr).
\end{eqnarray}
%
Thus we observe that this expected value is finite for all $v>0$ (also see \cite{koon,khar,zvai,xjon,marg}).
%\nocite{oreg,schn,pond,smith,marg,hunn,advi,koha,mouse}
#+end_src
*** back
#+begin_src sh :session *shell* :tangle vignettes/BiosmokeValidatedEvents_DataNote.tex :exports none :eval no :padline no

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{backmatter}

\section*{Competing interests}
  The authors declare that they have no competing interests.

\section*{Author's contributions}
    Text for this section \ldots

\section*{Acknowledgements}
  Text for this section \ldots
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file (bmc-mathphys, vancouver, spbasic).
\bibliography{bmc_article}      % Bibliography file (usually '*.bib' )
% for author-year bibliography (bmc-mathphys or spbasic)
% a) write to bib file (bmc-mathphys only)
% @settings{label, options="nameyear"}
% b) uncomment next line
%\nocite{label}

% or include bibliography directly:
% \begin{thebibliography}
% \bibitem{b1}
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files

\section*{Figures}
  \begin{figure}[h!]
  \caption{\csentence{Sample figure title.}
      A short description of the figure content
      should go here.}
      \end{figure}

\begin{figure}[h!]
  \caption{\csentence{Sample figure title.}
      Figure legend text.}
      \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Use of \listoftables is discouraged.
%%
\section*{Tables}
\begin{table}[h!]
\caption{Sample table title. This is where the description of the table should go.}
      \begin{tabular}{cccc}
        \hline
           & B1  &B2   & B3\\ \hline
        A1 & 0.1 & 0.2 & 0.3\\
        A2 & ... & ..  & .\\
        A3 & ..  & .   & .\\ \hline
      \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Additional Files}
  \subsection*{Additional file 1 --- Sample additional file title}
    Additional file descriptions text (including details of how to
    view the file, if it is in a non-standard format or the file extension).  This might
    refer to a multi-page table or a figure.

  \subsection*{Additional file 2 --- Sample additional file title}
    Additional file descriptions text.


\end{backmatter}
\end{document}

#+end_src
** Additional File 1
*** COMMENT AdditionalFile1-code
#+name:AdditionalFile1
#+begin_src R :session *shell* :tangle vignettes/BiosmokeValidatedEvents_AdditionalFile1.Rmd :exports none :eval no :padline no
---
title: "Biosmoke Validated Events Database Instructions for Accessing"
author: "Ivan C. Hanigan"
output:
  html_document:
    toc: true
documentclass: article    
---

# Introduction 

These instructions are for users to access the database to browse or download data.
A password is required and available on request to the corresponding author.

# Public access to data and potential for making contributions

## The dates of historical extreme events

The database holds air pollution data for several Australian cities that have had gaps in the records inmputed and a network-averaging procedure applied.  These events are based on the percentiles of daily observations from that pre-computed data and are available within the database.  Additional data may be provided to the database however a process of vetting the data prior to being imported will be required.  The original project team reserve the right to include new air pollution data at their own discretion.

The full list of events and their validation source reference identifier is available for public download as shown in the image below.

![img/web2py0.png](img/web2py0.png)

#+end_src
*** COMMENT deprecated ms access
#+begin_src R :session *shell* :tangle no :exports none :eval no :padline no

# MS Access

- Download and install the latest version of psqlODBC from: [http://www.postgresql.org/ftp/odbc/versions/msi/](http://www.postgresql.org/ftp/odbc/versions/msi/)
- Please note: If running a 64-bit version of Windows, you will want to download and install BOTH the psqlodbc_XX_XX_XXXX.zip and psqlodbc_XX_XX_XXXX-x64.zip file. 
- Assuming you use the 32 bit windows office suite (recommended) then use the command below to open the ODBC connections tool and add a DSN for your postgres database
- Go to Start -> Run (or Press Windows+R keys) and enter  the following (using 32 drivers even if you are on a 64 machine):

```
    %WINDIR%\SysWOW64\odbcad32.exe
```

- On either the User DSN or System DSN tab, click the Add button - find the PostgreSQL ODBC Driver.
- Select UNICODE, and click Finish.
- Data Source = biosmoke_events
- Database = ewedb
- Server = gislibrary.anu.edu.au

![img/mdbconnect.PNG](img/mdbconnect.PNG)

- User Name and Password will be supplied to you on request.
- Click the Save button to create the DSN.

#+end_src
*** COMMENT web2py
#+begin_src R :session *shell* :tangle vignettes/BiosmokeValidatedEvents_AdditionalFile1.Rmd :exports none :eval no :padline yes

## Adding reference material to the database

If a member of the research community has a known biomass smoke event date they can request authorisation to enter this data. The original project team reserve the right to include new validations for event dates at their own discretion. 

A member of the reseach community can make a request to be given the permission to edit the database.  These edits will be made to a staging area before validated events will be accepted by the database administrator and the master database is updated.

To make an addition the database is connected using an authenticated user with permission to write to the database then the Forms are used to first add a reference id (each report or satellite image has a unique ID)

![img/web2py1.png](img/web2py1.png)

- Then the event type, the dates and places that this reference validates are entered in the subform.
- There can be many events/places entered that are validated by a reference.

![img/web2py2.png](img/web2py2.png)

# Database access via pgAdmin for PostgreSQL management

- pgAdmin is recommended for advanced users
- Download and Install pgAdmin
- TODO fix the error link to brawn.anu to gislibrary.anu

![img/pgadminconnect.PNG](img/pgadminconnect.PNG)

![img/pgadminconnected.PNG](img/pgadminconnected.PNG)

# QGIS for Spatial Data Visualisation

- Download and install QGIS
- Click on the icon of the elephant

![img/qgis1.PNG](img/qgis1.PNG)

![img/qgis2.PNG](img/qgis2.PNG)

![img/qgis3.PNG](img/qgis3.PNG)

# Extending the Database using the Custom Built R Package

- The underlying pollution data from which extreme events are selected is created using a sequence of computational steps that create network averages of imputed daily pollution observations.
- The codes are all provided in an open source R package that interfaces with the database to create the pollution dataset.
- first download and install into your R library the rpostgrestools utility package [http://swish-climate-impact-assessment.github.io/rpostgrestools/](http://swish-climate-impact-assessment.github.com/rpostgrestools/)
- then Download and install the BiosmokeValidatedEvents R package [http://swish-climate-impact-assessment.github.io/BiosmokeValidatedEvents/](http://swish-climate-impact-assessment.github.com/BiosmokeValidatedEvents/)
- At the R console type

```{r, eval = F}
system.file(package = "BiosmokeValidatedEvents")
``` 

- This will return the location where the R package has been installed.  
- The scripts are in the `doc` directory, and the programs can be called from the `main.R` script found there.
- This assumes that there is an available PostGIS server called 'ewedb_staging' and that there is a table in that database called `biosmoke_pollution.combined_pollutants` that holds the new pollution data, along with spatial files `biosmoke_pollution.pollution_stations_combined_final` and `biosmoke_spatial.study_slas_01` that show the pollution monitoring stations and the Statistical Local Areas (SLA) that define the human populations of each study region.  



#+end_src
* data entry interface
** defaults for models/db.py
- might want to check in teh sqlite version if words are Reserved
- check_reserved=['all'],
#+begin_src markdown :tangle ~/tools/web2py/applications/biomass_smoke_events/models/db.py :exports none :eval no :padline no
  # -*- coding: utf-8 -*-
  
  #########################################################################
  ## This scaffolding model makes your app work on Google App Engine too
  ## File is released under public domain and you can use without limitations
  #########################################################################
  
  ## if SSL/HTTPS is properly configured and you want all HTTP requests to
  ## be redirected to HTTPS, uncomment the line below:
  # request.requires_https()
  
  if not request.env.web2py_runtime_gae:
      ## if NOT running on Google App Engine use SQLite or other DB
      db = DAL('sqlite://storage.sqlite',pool_size=1, fake_migrate_all = False)
      ## db = DAL("postgres://w2p_user:xpassword@localhost:5432/ewedb_staging", fake_migrate_all = False)
  else:
      ## connect to Google BigTable (optional 'google:datastore://namespace')
      db = DAL('google:datastore')
      ## store sessions and tickets there
      session.connect(request, response, db=db)
      ## or store session in Memcache, Redis, etc.
      ## from gluon.contrib.memdb import MEMDB
      ## from google.appengine.api.memcache import Client
      ## session.connect(request, response, db = MEMDB(Client()))
  
  ## by default give a view/generic.extension to all actions from localhost
  ## none otherwise. a pattern can be 'controller/function.extension'
  response.generic_patterns = ['*'] # if request.is_local else []
  ## (optional) optimize handling of static files
  # response.optimize_css = 'concat,minify,inline'
  # response.optimize_js = 'concat,minify,inline'
  ## (optional) static assets folder versioning
  # response.static_version = '0.0.0'
  #########################################################################
  ## Here is sample code if you need for
  ## - email capabilities
  ## - authentication (registration, login, logout, ... )
  ## - authorization (role based authorization)
  ## - services (xml, csv, json, xmlrpc, jsonrpc, amf, rss)
  ## - old style crud actions
  ## (more options discussed in gluon/tools.py)
  #########################################################################
  
  from gluon.tools import Auth, Crud, Service, PluginManager, prettydate
  auth = Auth(db)
  crud, service, plugins = Crud(db), Service(), PluginManager()
  
  ## create all tables needed by auth if not custom tables
  auth.define_tables(username=False, signature=False)
  
  ## configure email
  mail = auth.settings.mailer
  mail.settings.server = 'logging' or 'smtp.gmail.com:587'
  mail.settings.sender = 'you@gmail.com'
  mail.settings.login = 'username:password'
  
  ## configure auth policy
  auth.settings.registration_requires_verification = False
  auth.settings.registration_requires_approval = True
  auth.settings.reset_password_requires_verification = True
  
  ## if you need to use OpenID, Facebook, MySpace, Twitter, Linkedin, etc.
  ## register with janrain.com, write your domain:api_key in private/janrain.key
  from gluon.contrib.login_methods.rpx_account import use_janrain
  use_janrain(auth, filename='private/janrain.key')
  
  #########################################################################
  ## Define your tables below (or better in another model file) for example
  ##
  ## >>> db.define_table('mytable',Field('myfield','string'))
  ##
  ## Fields can be 'string','text','password','integer','double','boolean'
  ##       'date','time','datetime','blob','upload', 'reference TABLENAME'
  ## There is an implicit 'id integer autoincrement' field
  ## Consult manual for more options, validators, etc.
  ##
  ## More API examples for controllers:
  ##
  ## >>> db.mytable.insert(myfield='value')
  ## >>> rows=db(db.mytable.myfield=='value').select(db.mytable.ALL)
  ## >>> for row in rows: print row.id, row.myfield
  #########################################################################
  
  ## after defining tables, uncomment below to enable auditing
  # auth.enable_record_versioning(db)
#+end_src

** tables
*** COMMENT convert old table-code
#+name:convert old table
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:convert old table ####
  library(disentangle)
  
  setwd("~/projects/biomass_smoke_and_human_health/BiosmokeValidatedEvents/inst/extdata")
  tbls <- c("bio_events.tblreferences.csv",
  "bio_events.tblevents.csv",
  "bio_events.dust_event_records.csv",
  "bio_events.dust_event_records2.csv")
  dir()
  tb1  <- read.csv(tbls[1], stringsAsFactor = F)
  
  dd  <- data_dictionary(tb1)
  write.csv(dd, gsub(".csv", "_data_dictionary.csv", tbls[1]), row.names = F)
  

  
  
  tb2  <- read.csv(tbls[2], stringsAsFactor = F)
  
  dd  <- data_dictionary(tb2)
  write.csv(dd, gsub(".csv", "_data_dictionary.csv", tbls[2]), row.names = F)
  
#+end_src
*** COMMENT refs code
#+begin_src markdown :tangle ~/tools/web2py/applications/biomass_smoke_events/models/db.py :exports reports :eval no :padline no
    
  db.define_table(
      'biomass_smoke_reference',
      Field('source', 'string', comment='The source. Author or Organisation.'),
      Field('credentials', 'string', comment='Type of source.', requires=IS_IN_SET(['government','internet','journal','media','modis hotspot','modis smoke','toms'])),
      Field('year', 'integer', comment = 'Publication year'),
      Field('authors', 'string', comment= 'Author list.'),
      Field('title', 'string', comment= 'Reference title.'),
      Field('volume', 'integer', comment = 'Journal volume.'), 
      Field('general_location', 'string', comment= 'Free text location information.'),
      Field('url', 'string', comment= 'URL.'),
      Field('summary', 'text', comment= 'Short summary.'),
      Field('abstract', 'text', comment= 'Executive summary or journal abstract.'),
      format = '%(source)s %(id)s' 
      )
  
  db.biomass_smoke_reference.source.requires = IS_NOT_EMPTY()
  db.biomass_smoke_reference.year.requires = IS_NOT_EMPTY()

#+end_src
*** COMMENT inserts-code
#+name:inserts
#+begin_src R :session *shell* :tangle no :exports none :eval no
#### name:inserts ####
INSERT INTO public.biomass_smoke_reference(
            id, source, year,
authors,
title,
volume,

general_location,
credentials,
url,
summary,
abstract
)
select refid, field7, field4, 
field3,
field5,
field10, 

field32,
field28,
field39,
field29,
field37
from biosmoke_events.tblreferences;

#+end_src

*** COMMENT Events code
#+begin_src markdown :tangle ~/tools/web2py/applications/biomass_smoke_events/models/db.py :exports reports :eval no :padline no
    
  db.define_table(
      'biomass_smoke_event',
      Field('biomass_smoke_reference_id', db.biomass_smoke_reference),
      Field('place', requires = IS_IN_SET(['ALBANY','Albury','Bathurst','BUNBURY','BUSSELTON','GERALDTON','hobart','Illawarra','launceston','Newcastle','PERTH','Sydney East','Sydney West','Tamworth','Wagga Wagga']), comment='The pre-determined study locations of the biomass smoke project.'),
      Field('event_type', requires = IS_IN_SET(['bushfire','dust','non-biomass, fire','non-biomass, non-fire','possible biomass','prescribed burn','woodsmoke']), comment = 'Compulsory. Select from list'),
      Field('min_date', 'date', comment='The first date of known event. Compulsory.'),
      Field('max_date', 'date', comment = 'The last date of known event. Optional'),
      Field('burn_area_ha', 'double', comment = 'The area burnt in hectares'),
      Field('met_conditions', 'text', comment = 'Free text notes about relevant meteorological conditions'),
      format = '%(place)s %(id)s' 
      )
  
  db.biomass_smoke_event.min_date.requires = IS_NOT_EMPTY()

#+end_src
*** COMMENT inserts-code
#+name:inserts
#+begin_src R :session *shell* :tangle no :exports none :eval no
#### name:inserts ####
INSERT INTO public.biomass_smoke_event(
            id, 
biomass_smoke_reference_id,
place,
min_date,
max_date,
event_type,
burn_area_ha,
met_conditions
)
select eventid,
refid,
place,
mindate,
maxdate,
eventtype,
cast(burnareaha as numeric),
metconditions
from biosmoke_events.tblevents;

#+end_src

*** COMMENT put data into sqlite
#+name:transfer_to_sqlite
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:transfer_to_sqlite ####
  
  library(rpostgrestools)
  ch <- connect2postgres2("ewedb_staging")
  library(RSQLite)  
  drv <- dbDriver("SQLite")
  con <- dbConnect(drv, dbname = "~/tools/web2py/applications/biomass_smoke_events/databases/storage.sqlite")
  dbListTables(con)
  dat1 <- dbGetQuery(ch , "select * from biomass_smoke_reference")
  str(dat1)
  #dbSendQuery(con, "delete from biomass_smoke_reference")
  qc  <- dbGetQuery(con , "select * from biomass_smoke_reference")
  str(qc)
  
  dbWriteTable(con, "biomass_smoke_reference", dat1[,names(qc)], append = T, row.names=F)
  
  #dbSendQuery(con, "delete from biomass_smoke_event")
  qc2  <- dbGetQuery(con , "select * from biomass_smoke_event")
  str(qc2)
  dat2 <- dbGetQuery(ch , "select * from biomass_smoke_event")
  str(dat2)
  dat2$min_date <- as.character(dat2$min_date)
  dat2$max_date <- as.character(dat2$max_date)
  dbWriteTable(con, "biomass_smoke_event", dat2[,names(qc2)], append = T, row.names=F)
  
  dat1 <- dbGetQuery(ch , "select * from biosmoke_pollution.combined_pollutants")
  str(dat1)
  # SQLite does not have any native date types. store them as text. the SQLite date/time functions (like date()) act on a string, not a "date object" or integer
  dat1$date <- as.character(dat1$date)
  # dbRemoveTable(con, "combined_pollutants")
  #qc  <- dbGetQuery(con , "select * from combined_pollutants")
  #str(qc)
  dbWriteTable(con, "combined_pollutants", dat1, append = F, row.names=F)
  qc <- dbGetQuery(con, "select * from combined_pollutants where date > '2008-12-01'")
  table(qc$date)
  dbGetQuery(con, "select max(date) from combined_pollutants")
  # but WA licence precludes sharing without written consent so remove
  wa <- dbGetQuery(ch,
  "SELECT gid, nam, site, region, lat, lon
    FROM biosmoke_pollution.pollution_stations_combined_final 
    where nam = 'WESTERN AUSTRALIA'
  ")
  paste(wa$site, sep = "", collapse="','")
  
  dbGetQuery(con, "select site, count(*)
  from combined_pollutants
  group by site
  order by site
  ")
  
  dbSendQuery(con,
  "delete from combined_pollutants
  where site in ('Albany','Bunbury','Busselton','Geraldton','CavershamA',
  'CavershamB','Duncraig','QueensBuilding','QuinnsRocks','SouthLake')
  ")
  
  dbDisconnect(con)
#+end_src

** controllers
#+begin_src R :session *R* :tangle ~/tools/web2py/applications/biomass_smoke_events/controllers/default.py :exports none :eval no :padline no
  # -*- coding: utf-8 -*-
  # this file is released under public domain and you can use without limitations
  
  #########################################################################
  ## This is a sample controller
  ## - index is the default action of any application
  ## - user is required for authentication and authorization
  ## - download is for downloading files uploaded in the db (does streaming)
  ## - call exposes all registered services (none by default)
  #########################################################################
  
  
  def index():
      """
      example action using the internationalization operator T and flash
      rendered by views/default/index.html or views/generic.html
  
      if you need a simple wiki simply replace the two lines below with:
      return auth.wiki()
      """
      response.flash = T("Welcome to the Biomass Smoke Events Database!")
      return dict(message=T('This database contains validated bushfire smoke events in Australian cities.'))
   
  
  def user():
      """
      exposes:
      http://..../[app]/default/user/login
      http://..../[app]/default/user/logout
      http://..../[app]/default/user/register
      http://..../[app]/default/user/profile
      http://..../[app]/default/user/retrieve_password
      http://..../[app]/default/user/change_password
      http://..../[app]/default/user/manage_users (requires membership in
      use @auth.requires_login()
          @auth.requires_membership('group name')
          @auth.requires_permission('read','table name',record_id)
      to decorate functions that need access control
      """
      return dict(form=auth())
  
  @cache.action()
  def download():
      """
      allows downloading of uploaded files
      http://..../[app]/default/download/[filename]
      """
      return response.download(request, db)
  
  
  def call():
      """
      exposes services. for example:
      http://..../[app]/default/call/jsonrpc
      decorate with @services.jsonrpc the functions to expose
      supports xml, json, xmlrpc, jsonrpc, amfrpc, rss, csv
      """
      return service()
  
  
  @auth.requires_signature()
  def data():
      """
      http://..../[app]/default/data/tables
      http://..../[app]/default/data/create/[table]
      http://..../[app]/default/data/read/[table]/[id]
      http://..../[app]/default/data/update/[table]/[id]
      http://..../[app]/default/data/delete/[table]/[id]
      http://..../[app]/default/data/select/[table]
      http://..../[app]/default/data/search/[table]
      but URLs must be signed, i.e. linked with
        A('table',_href=URL('data/tables',user_signature=True))
      or with the signed load operator
        LOAD('default','data.load',args='tables',ajax=True,user_signature=True)
      """
      return dict(form=crud())

#+end_src

** forms
*** main refs
#+begin_src R :session *R* :tangle ~/tools/web2py/applications/biomass_smoke_events/controllers/forms.py :exports reports :eval no :padline no
  
  def manage_references():
      grid = SQLFORM.smartgrid(db.biomass_smoke_reference,linked_tables=['biomass_smoke_event'
                                                        ],
                               fields = [db.biomass_smoke_reference.id,
                                         db.biomass_smoke_reference.source,
                                         db.biomass_smoke_reference.credentials,
                                         db.biomass_smoke_reference.year,
                                         
                                         db.biomass_smoke_reference.title,
                                         db.biomass_smoke_event.place,
                                         db.biomass_smoke_event.min_date,
                                         db.biomass_smoke_event.event_type
                                         ],
                                         orderby = dict(id=db.biomass_smoke_reference.id, place=db.biomass_smoke_event.place),
                               user_signature=True,maxtextlength =50, csv=False, paginate=35)
      return dict(grid=grid)
#+end_src  

*** view Dates

#+begin_src R :session *R* :tangle ~/tools/web2py/applications/biomass_smoke_events/controllers/forms.py :exports reports :eval no :padline no
  
  def manage_events():
      grid = SQLFORM.smartgrid(db.biomass_smoke_event,linked_tables=['biomass_smoke_reference'
                                                        ],
                               fields = [db.biomass_smoke_event.id,
                                         db.biomass_smoke_event.place,
                                         db.biomass_smoke_event.min_date,
                                         db.biomass_smoke_event.event_type,
                                         db.biomass_smoke_event.biomass_smoke_reference_id,
                                         db.biomass_smoke_reference.id,
                                         db.biomass_smoke_reference.source,
                                         db.biomass_smoke_reference.credentials,
                                         db.biomass_smoke_reference.year,
                                         
                                         db.biomass_smoke_reference.title                                       
                                         ],
                                         orderby = dict(place=db.biomass_smoke_event.place, min_date=db.biomass_smoke_event.min_date),
                               user_signature=True,maxtextlength =50, csv=True, paginate=35)
      return dict(grid=grid)
#+end_src  

** views
#+begin_src markdown :tangle ~/tools/web2py/applications/biomass_smoke_events/views/default/index.html :exports reports :eval no :padline
    {{left_sidebar_enabled,right_sidebar_enabled=False,('message' in globals())}}
{{extend 'layout.html'}}
    
  {{if 'message' in globals():}}
    <h3>{{=message}}</h3>

    <h4>{{=T('Instructions')}}</h4>
<ol>
      <li>{{=T('You are using the events database')}}</li>
  <li>{{=XML(T('The list of validated events is at %s',
               A('%(application)s/forms/manage_events/'%request,
           _href=URL('forms','manage_events'))))}}</li>
  
  <li>{{=XML(T('The main tool for managing references that validate events is at %s',
               A('%(application)s/forms/manage_references/'%request,
           _href=URL('forms','manage_references'))))}}</li>
    <li>{{=T('To become a user of the data inventory please register in top right corner, then email ivan.hanigan at gmail.com requesting approval.')}}</li>
</ol>
    {{elif 'content' in globals():}}
{{=content}}
    {{else:}}
{{=BEAUTIFY(response._vars)}}
    {{pass}}

    {{block right_sidebar}}
{{=A(T("About this site"), _href=URL('static', 'index.html'), _class='btn',
         _style='margin-top: 1em;')}}
{{end}}
    
#+end_src

** about
#+begin_src R :session *R* :tangle no :exports reports :eval no :padline no
  dir()
  system("pandoc -i README.md -o index.html")
  file.rename("index.html", "~/tools/web2py/applications/biomass_smoke_events/static/index.html")
#+end_src
** scp
rsync -avz -e "ssh -i /home/ivan_hanigan/tools/macpro_ssh/.ssh/id_rsa" /home/ivan_hanigan/tools/web2py/applications/biomass_smoke_events/ ivan@gislibrary.anu.edu.au:/home/ivan/web2py/applications/biomass_smoke_events/ 
* Backups
*** COMMENT backups-code
**** COMMENT pgdump-local
#+name:pgdump
#+begin_src R :session *R* :tangle no :exports none :eval yes
  cat(sprintf('
  pg_dump -h gislibrary.anu.edu.au -p 5432 -U postgres -W -F t -v -i -f "/home/ivan_hanigan/projects/biomass_smoke_and_human_health/BiosmokeValidatedEvents_backups/backup_file-%s.backup" -n "public" ewedb
  ',gsub(":","-",gsub(" ","-",Sys.time()))))
    
#+end_src

#+RESULTS: pgdump

